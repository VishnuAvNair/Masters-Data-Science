{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension 1 - - Adding POS tags tags to the input\n",
    "\n",
    "Implementing solution described in the paper below, using a mixed text + POS TAG features: \n",
    "\"Franck Dernoncourt, Ji Young Lee, and Peter\n",
    "Szolovits. 2016. Neural networks for joint sentence\n",
    "classification in medical paper abstracts. European\n",
    "Chapter of the Association for Computational Linguistics\n",
    "(EACL) 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Tokens (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \n",
       "0  OBJECTIVE  \n",
       "1    METHODS  \n",
       "2    METHODS  \n",
       "3    METHODS  \n",
       "4    METHODS  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file PubMed_20k_RCT.csv created by script01_create_single_dataset\n",
    "df_all_text = pd.read_csv('input/PubMed_20k_RCT.csv')\n",
    "df_train_text = df_all_text[df_all_text['partition']=='train']\n",
    "df_valid_text = df_all_text[df_all_text['partition']=='dev']\n",
    "df_test_text = df_all_text[df_all_text['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train partition size: 180040\n",
      "Valid partition size: 30212\n",
      "Test partition size: 30135\n",
      "Total dataset size: 240387\n"
     ]
    }
   ],
   "source": [
    "X_train_cnt = df_train_text.shape[0]\n",
    "X_valid_cnt = df_valid_text.shape[0]\n",
    "X_test_cnt = df_test_text.shape[0]\n",
    "\n",
    "X_all_text = df_all_text.text.values\n",
    "\n",
    "print('Train partition size: {}'.format(X_train_cnt))\n",
    "print('Valid partition size: {}'.format(X_valid_cnt))\n",
    "print('Test partition size: {}'.format(X_test_cnt))\n",
    "print('Total dataset size: {}'.format(X_all_text.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - POS TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>postaglist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>PART VERB DET NOUN ADP NUM NOUN ADP ADJ ADJ PUNCT NOUN ADJ NOUN ADP VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ ADJ PUNCT NOUN NOUN ADP DET ADJ NOUN CCONJ ADP DET NOUN VERB VERB VERB ADP NUM NOUN ADP ADJ NOUN ADP ADJ ADP ADJ NOUN NOUN PUNCT PROPN PUNCT PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>DET NOUN ADP NUM NOUN ADP ADJ NOUN PROPN VERB VERB NUM PUNCT NUM VERB NUM NOUN SYM NOUN ADP NOUN CCONJ NUM VERB NOUN ADP NUM NOUN PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>PROPN NOUN VERB NOUN NOUN CCONJ NOUN ADP NOUN NOUN CCONJ ADJ NOUN NOUN PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>PROPN VERB VERB VERB DET ADJ NOUN NOUN NOUN PUNCT NUM SYM NUM NOUN PUNCT PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>ADJ NOUN NOUN VERB DET PROPN PROPN CCONJ PROPN PROPN PROPN PROPN NOUN PUNCT ADJ ADJ NOUN PUNCT PROPN PUNCT ADP DET NOUN ADP NOUN PROPN PUNCT CCONJ NUM NOUN NOUN PUNCT NOUN PUNCT PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \\\n",
       "0  OBJECTIVE   \n",
       "1    METHODS   \n",
       "2    METHODS   \n",
       "3    METHODS   \n",
       "4    METHODS   \n",
       "\n",
       "                                                                                                                                                                                                                                                    postaglist  \n",
       "0  PART VERB DET NOUN ADP NUM NOUN ADP ADJ ADJ PUNCT NOUN ADJ NOUN ADP VERB NOUN PUNCT NOUN PUNCT CCONJ ADJ ADJ PUNCT NOUN NOUN ADP DET ADJ NOUN CCONJ ADP DET NOUN VERB VERB VERB ADP NUM NOUN ADP ADJ NOUN ADP ADJ ADP ADJ NOUN NOUN PUNCT PROPN PUNCT PUNCT  \n",
       "1                                                                                                                      DET NOUN ADP NUM NOUN ADP ADJ NOUN PROPN VERB VERB NUM PUNCT NUM VERB NUM NOUN SYM NOUN ADP NOUN CCONJ NUM VERB NOUN ADP NUM NOUN PUNCT  \n",
       "2                                                                                                                                                                                 PROPN NOUN VERB NOUN NOUN CCONJ NOUN ADP NOUN NOUN CCONJ ADJ NOUN NOUN PUNCT  \n",
       "3                                                                                                                                                                               PROPN VERB VERB VERB DET ADJ NOUN NOUN NOUN PUNCT NUM SYM NUM NOUN PUNCT PUNCT  \n",
       "4                                                                      ADJ NOUN NOUN VERB DET PROPN PROPN CCONJ PROPN PROPN PROPN PROPN NOUN PUNCT ADJ ADJ NOUN PUNCT PROPN PUNCT ADP DET NOUN ADP NOUN PROPN PUNCT CCONJ NUM NOUN NOUN PUNCT NOUN PUNCT PUNCT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file PubMed_20k_RCT.csv created by script01_create_single_dataset\n",
    "df_all_postag = pd.read_csv('input/PubMed_20k_RCT_POS_TAG.csv')\n",
    "df_train_postag = df_all_postag[df_all_postag['partition']=='train']\n",
    "df_valid_postag = df_all_postag[df_all_postag['partition']=='dev']\n",
    "df_test_postag = df_all_postag[df_all_postag['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all_postag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cnt2 = df_train_postag.shape[0]\n",
    "X_valid_cnt2 = df_valid_postag.shape[0]\n",
    "X_test_cnt2 = df_test_postag.shape[0]\n",
    "\n",
    "X_all_postag = df_all_postag.postaglist.values\n",
    "\n",
    "assert X_train_cnt2 == X_train_cnt\n",
    "assert X_valid_cnt2 == X_valid_cnt\n",
    "assert X_test_cnt2 == X_test_cnt\n",
    "assert X_all_text.shape[0] == X_all_postag.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Create Vector Representation (dim=100) for Text and POS TAG Sentences\n",
    "\n",
    "I will obtain a vectorized representation of dim = 100 of each sentence in it's text form and it's pos tag list form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Get Vector Representation for Text\n",
    "### 2.1.1 - Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 67356\n",
      "CPU times: user 12.2 s, sys: 119 ms, total: 12.4 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_text = Tokenizer()\n",
    "tokenizer_text.fit_on_texts(X_all_text)\n",
    "sequences_text = tokenizer_text.texts_to_sequences(X_all_text)\n",
    "\n",
    "word_index_text = tokenizer_text.word_index\n",
    "VOC_SIZE_text = len(word_index_text)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 50)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN_text = 50\n",
    "X_seq_all_text = pad_sequences(sequences_text, maxlen=MAX_SEQ_LEN_text, dtype='int32', \\\n",
    "                               padding='pre', truncating='post', value=0)\n",
    "print(X_seq_all_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Get vector represetation of sentence text using LSMT MLP model\n",
    "\n",
    "#### 2.1.2.1 Retrieve pre-trained LSMT MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 200)           13471400  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 800)               3203200   \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               80100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_2 (Dense)    (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 16,756,825\n",
      "Trainable params: 3,285,425\n",
      "Non-trainable params: 13,471,400\n",
      "_________________________________________________________________\n",
      "CPU times: user 6.37 s, sys: 531 ms, total: 6.9 s\n",
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read model that creates vector representation of sentences\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_text = load_model('input/Baseline_Part1_LSTM_MPL.h5')\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 Get vector with dim = 100 representing sentence text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model_text.input, outputs=model_text.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train_text = intermediate_layer_model.predict(X_seq_all_text[:X_train_cnt])\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid_text = intermediate_layer_model.predict(X_seq_all_text[X_train_cnt:(X_train_cnt+X_valid_cnt)])\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test_text = intermediate_layer_model.predict(X_seq_all_text[-X_test_cnt:])\n",
    "\n",
    "# concat all data\n",
    "sentence_all_text = np.vstack((sentence_train_text, sentence_valid_text, sentence_test_text))\n",
    "\n",
    "print(sentence_train_text.shape)\n",
    "print(sentence_valid_text.shape)\n",
    "print(sentence_test_text.shape)\n",
    "print(sentence_all_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Get Vector Representation for POS TAG\n",
    "### 2.2.1 - Tokenize POS TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_postag = Tokenizer()\n",
    "tokenizer_postag.fit_on_texts(X_all_postag)\n",
    "sequences_postag = tokenizer_postag.texts_to_sequences(X_all_postag)\n",
    "\n",
    "word_index_postag = tokenizer_postag.word_index\n",
    "VOC_SIZE_postag = len(word_index_postag)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE_postag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN_postag = 120\n",
    "X_seq_all_postag = pad_sequences(sequences_postag, maxlen=MAX_SEQ_LEN_postag, dtype='int32', \\\n",
    "                               padding='pre', truncating='post', value=0)\n",
    "print(X_seq_all_postag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - One hot encode POS TAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_all_postag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_DIM_postag = VOC_SIZE_postag\n",
    "\n",
    "def get_vector(token):\n",
    "    vector = np.zeros(EMBEDDING_DIM_postag)\n",
    "    if token != 0:\n",
    "        vector[token-1] = 1\n",
    "    return vector\n",
    "\n",
    "X_seq_all_postag_v = np.zeros((X_seq_all_postag.shape[0], X_seq_all_postag.shape[1] * EMBEDDING_DIM_postag))\n",
    "print(X_seq_all_postag_v.shape)\n",
    "i = 0\n",
    "for token_seq in X_seq_all_postag:\n",
    "    X_seq_all_postag_v[i] = np.array([get_vector(token) for token in token_seq]).flatten()\n",
    "    i += 1\n",
    "\n",
    "X_seq_all_postag_v = X_seq_all_postag_v.reshape(X_seq_all_postag_v.shape[0], \\\n",
    "                                               int(X_seq_all_postag_v.shape[1]/EMBEDDING_DIM_postag), \\\n",
    "                                               EMBEDDING_DIM_postag)\n",
    "print(X_seq_all_postag_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Get vector represetation of POS TAG text using pre-trained ConvD1 model\n",
    "\n",
    "#### 2.2.3.1 Retrieve pre-trained POS TAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read model that creates vector representation of sentences\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_postag = load_model('input/Extension_Part1_CONV1_POS_TAG2.h5')\n",
    "\n",
    "model_postag.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.2 Get vector with dim = 100 representing sentence POS TAG sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model_postag.input, outputs=model_postag.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train_postag = intermediate_layer_model.predict(X_seq_all_postag_v[:X_train_cnt])\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid_postag = intermediate_layer_model.predict(X_seq_all_postag_v[X_train_cnt:(X_train_cnt+X_valid_cnt)])\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test_postag = intermediate_layer_model.predict(X_seq_all_postag_v[-X_test_cnt:])\n",
    "\n",
    "# concat all data\n",
    "sentence_all_postag = np.vstack((sentence_train_postag, sentence_valid_postag, sentence_test_postag))\n",
    "\n",
    "print(sentence_train_postag.shape)\n",
    "print(sentence_valid_postag.shape)\n",
    "print(sentence_test_postag.shape)\n",
    "print(sentence_all_postag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Vectorize output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "labels = df_all_text.label.values\n",
    "label_dict = {label: no for no, label in enumerate(set(labels))}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])\n",
    "\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Combine Text and POS TAG representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comb = np.hstack((sentence_train_text, sentence_train_postag))\n",
    "X_valid_comb = np.hstack((sentence_valid_text, sentence_valid_postag))\n",
    "X_test_comb = np.hstack((sentence_test_text, sentence_test_postag))\n",
    "print(X_train_comb.shape)\n",
    "print(X_valid_comb.shape)\n",
    "print(X_test_comb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train combined model (text and POS TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "comb = Input(shape=(200,), dtype='float32')\n",
    "D1 = Dense(units=200, activation='relu')(comb)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "D1 = Dense(units=100, activation='relu', name='sentence_vector_1')(D1)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "D1 = Dense(units=20, activation='relu')(D1)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "out = Dense(5,\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "            activity_regularizer=regularizers.l1(0.01),\n",
    "            activation=\"softmax\")(D1) \n",
    "\n",
    "model = Model(comb, out)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# learn\n",
    "model.fit(X_train_comb, y_all[:X_train_cnt], \\\n",
    "          validation_data=(X_valid_comb, y_all[X_train_cnt:(X_train_cnt+X_valid_cnt)]), \\\n",
    "          callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "          verbose=1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6 - Save combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('input/Extension_Part1_LSTM_MLP_Glove_CONV1_PosTag.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create sentence and label sequences\n",
    "\n",
    "### 7.1 - Find max length of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find size of abstracts\n",
    "\n",
    "# create dataframe with abstract lengths\n",
    "abstract_lens = df_all_text[['abstract_id','text']].groupby(by='abstract_id').agg('count')\n",
    "\n",
    "# create dict with abstract lengths\n",
    "abstract_len_dict = {abstract_id: length for abstract_id, length in abstract_lens.reset_index().values}\n",
    "\n",
    "print('Total number of abstracts = {}'.format(len(abstract_lens.index)))\n",
    "print('Max abstract size = {}'.format(np.max(abstract_lens.text.values)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "axes[0].boxplot(abstract_lens.text.values)\n",
    "axes[1].hist(abstract_lens.text.values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Redo vectorization of output labels - add 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_labels = df_all_text.label.values\n",
    "\n",
    "# create label set and add 'PAD' to the set\n",
    "label_set = set(y_labels)\n",
    "label_set.add('PAD')\n",
    "label_dict = {label: no for no, label in enumerate(label_set)}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get inverted dict\n",
    "label_dict_inv = {no: label for label, no in label_dict.items()}\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in y_labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])\n",
    "\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 - Get vectorized representation of sentence text + POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train = intermediate_layer_model.predict(X_train_comb)\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid = intermediate_layer_model.predict(X_valid_comb)\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test = intermediate_layer_model.predict(X_test_comb)\n",
    "\n",
    "# concat all data\n",
    "sentence_all = np.vstack((sentence_train, sentence_valid, sentence_test))\n",
    "\n",
    "print(sentence_train.shape)\n",
    "print(sentence_valid.shape)\n",
    "print(sentence_test.shape)\n",
    "print(sentence_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 - Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get abstract and label sequences\n",
    "\n",
    "def get_abstract_seq(sentence_arry, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    sentences = sentence_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.zeros(100 * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_abstract = np.hstack((left_padding, sentences))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_abstract) == 3100\n",
    "    return full_abstract\n",
    "\n",
    "def get_label_seq(label_arry, pad, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    labels = label_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.array(list(pad) * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_labels = np.hstack((left_padding, labels))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_labels) == 31 * 6\n",
    "    return full_labels\n",
    "\n",
    "# create lists to store abstracts and label sequence for abstracts\n",
    "abstracts = list()\n",
    "labels = list()\n",
    "sorted_abs_id = list()\n",
    "\n",
    "# find categorial \n",
    "y_PAD = to_categorical(label_dict['PAD'], num_classes=number_of_classes)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    \n",
    "    # find number of sentences in abstract\n",
    "    abs_id = df_all_text.abstract_id.values[i]\n",
    "    # save district abstract ids, in the order they appear in the dataset\n",
    "    sorted_abs_id.append(abs_id)\n",
    "    abs_len = abstract_len_dict[abs_id]\n",
    "    j = i + abs_len\n",
    "    \n",
    "    # get flattened sentences that make up abstract\n",
    "    abstracts.append(get_abstract_seq(sentence_all, i, j))\n",
    "    \n",
    "    # get flattened labels that make up abstract labels\n",
    "    labels.append(get_label_seq(y_all, y_PAD, i, j))\n",
    "    \n",
    "    i = j\n",
    "    if j >= len(df_all_text.index):\n",
    "        print('Done!')\n",
    "        break\n",
    "        \n",
    "print('Number of abstracts: {}'.format(len(abstracts)))\n",
    "print('Number of label seqs: {}'.format(len(labels)))\n",
    "\n",
    "print('\\nNumber of abstracts in each partition: \\n{}'.format(df_all_text.groupby(by=('partition')).abstract_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore partition data\n",
    "\n",
    "X_all = np.array(abstracts)\n",
    "X_all = X_all.reshape(X_all.shape[0],31,100)\n",
    "X_train = X_all[:15000]\n",
    "X_valid = X_all[15000:17500]\n",
    "X_test  = X_all[-2500:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_seq_all = np.array(labels)\n",
    "y_seq_all = y_seq_all.reshape(y_seq_all.shape[0],31,6)\n",
    "y_seq_train = y_seq_all[:15000]\n",
    "y_seq_valid = y_seq_all[15000:17500]\n",
    "y_seq_test  = y_seq_all[-2500:]\n",
    "\n",
    "print(y_seq_train.shape)\n",
    "print(y_seq_valid.shape)\n",
    "print(y_seq_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "abs = Input(shape=(X_all.shape[1], X_all.shape[2]), dtype='float32')\n",
    "lstm = Bidirectional(LSTM(units=100, return_sequences=True))(abs)\n",
    "out = TimeDistributed(Dense(6, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model_final = Model(abs, out)\n",
    "model_final.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_final.summary()\n",
    "\n",
    "# learn\n",
    "model_final.fit(X_train, y_seq_train, validation_data=(X_valid, y_seq_valid), \\\n",
    "            callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "            verbose=1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.save('input/Extension_final_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Final prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = model_final.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1).flatten()\n",
    "\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1).flatten()\n",
    "\n",
    "acc = accuracy_score(y_seq_true, y_seq_hat)\n",
    "print('Accuracy on test partition (padded label sequence): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_final.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1)\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1)\n",
    "\n",
    "#print(y_seq_hat.shape)\n",
    "#print(y_seq_true.shape)\n",
    "\n",
    "y_seq_hat_trimmed = list()\n",
    "y_seq_true_trimmed = list()\n",
    "i = 0\n",
    "for abs_id in sorted_abs_id[-2500:]:\n",
    "    x = list(y_seq_hat[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_hat_trimmed.insert(0, x.pop())\n",
    "    x = list(y_seq_true[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_true_trimmed.insert(0, x.pop())    \n",
    "    i += 1\n",
    "    \n",
    "#print(len(y_seq_hat_trimmed))\n",
    "#print(len(y_seq_true_trimmed))\n",
    "\n",
    "acc = accuracy_score(y_seq_true_trimmed, y_seq_hat_trimmed)\n",
    "print('Accuracy on test partition (no padding): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_seq_true_trimmed, y_seq_hat_trimmed, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
