{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (part 2)\n",
    "\n",
    "Implementing second part of solution described here: \n",
    "\"Franck Dernoncourt, Ji Young Lee, and Peter\n",
    "Szolovits. 2016. Neural networks for joint sentence\n",
    "classification in medical paper abstracts. European\n",
    "Chapter of the Association for Computational Linguistics\n",
    "(EACL) 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \n",
       "0  OBJECTIVE  \n",
       "1    METHODS  \n",
       "2    METHODS  \n",
       "3    METHODS  \n",
       "4    METHODS  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file PubMed_20k_RCT.csv created by script01_create_single_dataset\n",
    "df_all = pd.read_csv('input/PubMed_20k_RCT.csv')\n",
    "df_train = df_all[df_all['partition']=='train']\n",
    "df_valid = df_all[df_all['partition']=='dev']\n",
    "df_test = df_all[df_all['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train partition size: 180040\n",
      "Valid partition size: 30212\n",
      "Test partition size: 30135\n",
      "Total dataset size: 240387\n"
     ]
    }
   ],
   "source": [
    "X_train_cnt = df_train.shape[0]\n",
    "X_valid_cnt = df_valid.shape[0]\n",
    "X_test_cnt = df_test.shape[0]\n",
    "\n",
    "X_all = df_all.text.values\n",
    "\n",
    "print('Train partition size: {}'.format(X_train_cnt))\n",
    "print('Valid partition size: {}'.format(X_valid_cnt))\n",
    "print('Test partition size: {}'.format(X_test_cnt))\n",
    "print('Total dataset size: {}'.format(X_all.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 67356\n",
      "CPU times: user 26.5 s, sys: 265 ms, total: 26.8 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# reduce vocabulary size to make problem manageable for available computing resources\n",
    "#SEQ_VOC = 50000\n",
    "#print('Number of tokens for sequences = {}'.format(SEQ_VOC))\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=VOC_SIZE, filters='!\"*,./:;?@\\`|')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_all)\n",
    "sequences = tokenizer.texts_to_sequences(X_all)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "VOC_SIZE = len(word_index)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "X_token_seq_all = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='pre', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 6)\n",
      "CPU times: user 4.61 s, sys: 174 ms, total: 4.79 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_labels = df_all.label.values\n",
    "\n",
    "# create label set and add 'PAD' to the set\n",
    "label_set = set(y_labels)\n",
    "label_set.add('PAD')\n",
    "label_dict = {label: no for no, label in enumerate(label_set)}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get inverted dict\n",
    "label_dict_inv = {no: label for label, no in label_dict.items()}\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in y_labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])\n",
    "\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embedding layer with pre-trained glove word vectors\n",
    "\n",
    "I use glove pre-trained (on Twiter) embeddings. <br> \n",
    "My code follows guidelines from Keras tutorial: <br>\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding dim = 200\n",
      "Number of word vectors found: 1193514\n",
      "CPU times: user 2min 28s, sys: 5.81 s, total: 2min 34s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read pre-trained glove embeddings\n",
    "\n",
    "# create dictionary with word embeddings\n",
    "embedding_file = {}\n",
    "embedding_file[25] = 'glove/glove.twitter.27B.25d.txt'\n",
    "embedding_file[50] = 'glove/glove.twitter.27B.50d.txt'\n",
    "embedding_file[100] = 'glove/glove.twitter.27B.100d.txt'\n",
    "embedding_file[200] = 'glove/glove.twitter.27B.200d.txt'\n",
    "\n",
    "# set embedding dimension\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Using embedding dim = {}'.format(EMBEDDING_DIM))\n",
    "\n",
    "f = open(embedding_file[EMBEDDING_DIM], 'r')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Number of word vectors found: {}'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 284 ms, sys: 410 ms, total: 693 ms\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create matrix with embedding coefs for each word in vocabulary\n",
    "# row position of word representation in the matrix will be the word number\n",
    "\n",
    "# initiazes matrix with zeros\n",
    "embedding_matrix = np.zeros((VOC_SIZE + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dense vector representation of sentences for training\n",
    "\n",
    "First will train part 2 of the model by obtaining output from intermediary layer from model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 200)           13471400  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1600)              6406400   \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               160100    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_2 (Dense)    (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 20,040,025\n",
      "Trainable params: 6,568,625\n",
      "Non-trainable params: 13,471,400\n",
      "_________________________________________________________________\n",
      "CPU times: user 19.9 s, sys: 1.3 s, total: 21.2 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read model that creates vector representation of sentences\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_1 = load_model('input/Baseline_Part1_Bi-LSTM_MLP.h5')\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 100)\n",
      "(30212, 100)\n",
      "(30135, 100)\n",
      "(240387, 100)\n",
      "CPU times: user 5h 2min 35s, sys: 13min 13s, total: 5h 15min 49s\n",
      "Wall time: 1h 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model_1.input, outputs=model_1.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train = intermediate_layer_model.predict(X_token_seq_all[:X_train_cnt])\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid = intermediate_layer_model.predict(X_token_seq_all[X_train_cnt:(X_train_cnt+X_valid_cnt)])\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test = intermediate_layer_model.predict(X_token_seq_all[-X_test_cnt:])\n",
    "\n",
    "# concat all data\n",
    "sentence_all = np.vstack((sentence_train, sentence_valid, sentence_test))\n",
    "\n",
    "print(sentence_train.shape)\n",
    "print(sentence_valid.shape)\n",
    "print(sentence_test.shape)\n",
    "print(sentence_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentence and label sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of abstracts = 20000\n",
      "Max abstract size = 31\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAD8CAYAAAAG/FfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEnqMf8fOwQBSugfdSwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1835bd2d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find size of abstracts\n",
    "\n",
    "# create dataframe with abstract lengths\n",
    "abstract_lens = df_all[['abstract_id','text']].groupby(by='abstract_id').agg('count')\n",
    "\n",
    "# create dict with abstract lengths\n",
    "abstract_len_dict = {abstract_id: length for abstract_id, length in abstract_lens.reset_index().values}\n",
    "\n",
    "print('Total number of abstracts = {}'.format(len(abstract_lens.index)))\n",
    "print('Max abstract size = {}'.format(np.max(abstract_lens.text.values)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "axes[0].boxplot(abstract_lens.text.values)\n",
    "axes[1].hist(abstract_lens.text.values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Number of abstracts: 20000\n",
      "Number of label seqs: 20000\n",
      "\n",
      "Number of abstracts in each partition: \n",
      "partition\n",
      "dev       2500\n",
      "test      2500\n",
      "train    15000\n",
      "Name: abstract_id, dtype: int64\n",
      "CPU times: user 1.19 s, sys: 229 ms, total: 1.42 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get abstract and label sequences\n",
    "\n",
    "def get_abstract_seq(sentence_arry, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    sentences = sentence_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.zeros(100 * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_abstract = np.hstack((left_padding, sentences))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_abstract) == 3100\n",
    "    return full_abstract\n",
    "\n",
    "def get_label_seq(label_arry, pad, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    labels = label_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.array(list(pad) * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_labels = np.hstack((left_padding, labels))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_labels) == 31 * 6\n",
    "    return full_labels\n",
    "\n",
    "# create lists to store abstracts and label sequence for abstracts\n",
    "abstracts = list()\n",
    "labels = list()\n",
    "sorted_abs_id = list()\n",
    "\n",
    "# find categorial \n",
    "y_PAD = to_categorical(label_dict['PAD'], num_classes=number_of_classes)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    \n",
    "    # find number of sentences in abstract\n",
    "    abs_id = df_all.abstract_id.values[i]\n",
    "    # save district abstract ids, in the order they appear in the dataset\n",
    "    sorted_abs_id.append(abs_id)\n",
    "    abs_len = abstract_len_dict[abs_id]\n",
    "    j = i + abs_len\n",
    "    \n",
    "    # get flattened sentences that make up abstract\n",
    "    abstracts.append(get_abstract_seq(sentence_all, i, j))\n",
    "    \n",
    "    # get flattened labels that make up abstract labels\n",
    "    labels.append(get_label_seq(y_all, y_PAD, i, j))\n",
    "    \n",
    "    i = j\n",
    "    if j >= len(df_all.index):\n",
    "        print('Done!')\n",
    "        break\n",
    "        \n",
    "print('Number of abstracts: {}'.format(len(abstracts)))\n",
    "print('Number of label seqs: {}'.format(len(labels)))\n",
    "\n",
    "print('\\nNumber of abstracts in each partition: \\n{}'.format(df_all.groupby(by=('partition')).abstract_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 31, 100)\n",
      "(2500, 31, 100)\n",
      "(2500, 31, 100)\n",
      "(15000, 31, 6)\n",
      "(2500, 31, 6)\n",
      "(2500, 31, 6)\n"
     ]
    }
   ],
   "source": [
    "# restore partition data\n",
    "\n",
    "X_all = np.array(abstracts)\n",
    "X_all = X_all.reshape(X_all.shape[0],31,100)\n",
    "X_train = X_all[:15000]\n",
    "X_valid = X_all[15000:17500]\n",
    "X_test  = X_all[-2500:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_seq_all = np.array(labels)\n",
    "y_seq_all = y_seq_all.reshape(y_seq_all.shape[0],31,6)\n",
    "y_seq_train = y_seq_all[:15000]\n",
    "y_seq_valid = y_seq_all[15000:17500]\n",
    "y_seq_test  = y_seq_all[-2500:]\n",
    "\n",
    "print(y_seq_train.shape)\n",
    "print(y_seq_valid.shape)\n",
    "print(y_seq_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 31, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 31, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 31, 6)             1206      \n",
      "=================================================================\n",
      "Total params: 162,006\n",
      "Trainable params: 162,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15000 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 14s 919us/step - loss: 0.1598 - acc: 0.9556 - val_loss: 0.1098 - val_acc: 0.9597\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 12s 781us/step - loss: 0.0765 - acc: 0.9700 - val_loss: 0.1041 - val_acc: 0.9611\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 12s 805us/step - loss: 0.0719 - acc: 0.9713 - val_loss: 0.1008 - val_acc: 0.9638\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 12s 814us/step - loss: 0.0694 - acc: 0.9718 - val_loss: 0.0981 - val_acc: 0.9635\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 13s 836us/step - loss: 0.0668 - acc: 0.9730 - val_loss: 0.0967 - val_acc: 0.9636\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 13s 873us/step - loss: 0.0659 - acc: 0.9733 - val_loss: 0.0965 - val_acc: 0.9634\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 14s 935us/step - loss: 0.0643 - acc: 0.9741 - val_loss: 0.0975 - val_acc: 0.9647\n",
      "CPU times: user 9min, sys: 44 s, total: 9min 44s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "abs = Input(shape=(X_all.shape[1], X_all.shape[2]), dtype='float32')\n",
    "lstm = Bidirectional(LSTM(units=100, return_sequences=True))(abs)\n",
    "out = TimeDistributed(Dense(6, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model_2 = Model(abs, out)\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "# learn\n",
    "model_2.fit(X_train, y_seq_train, validation_data=(X_valid, y_seq_valid), \\\n",
    "          callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "          verbose=1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_2.save('input/Baseline_Part2_Bi-LSTM_MPL_BLSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test partition (padded label sequence): 0.9623612903225807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = model_2.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1).flatten()\n",
    "\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1).flatten()\n",
    "\n",
    "acc = accuracy_score(y_seq_true, y_seq_hat)\n",
    "print('Accuracy on test partition (padded label sequence): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test partition (no padding): 0.9032022565123611\n"
     ]
    }
   ],
   "source": [
    "preds = model_2.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1)\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1)\n",
    "\n",
    "#print(y_seq_hat.shape)\n",
    "#print(y_seq_true.shape)\n",
    "\n",
    "y_seq_hat_trimmed = list()\n",
    "y_seq_true_trimmed = list()\n",
    "i = 0\n",
    "for abs_id in sorted_abs_id[-2500:]:\n",
    "    x = list(y_seq_hat[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_hat_trimmed.insert(0, x.pop())\n",
    "    x = list(y_seq_true[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_true_trimmed.insert(0, x.pop())    \n",
    "    i += 1\n",
    "    \n",
    "#print(len(y_seq_hat_trimmed))\n",
    "#print(len(y_seq_true_trimmed))\n",
    "\n",
    "acc = accuracy_score(y_seq_true_trimmed, y_seq_hat_trimmed)\n",
    "print('Accuracy on test partition (no padding): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90320225651236108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_seq_true_trimmed, y_seq_hat_trimmed, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
