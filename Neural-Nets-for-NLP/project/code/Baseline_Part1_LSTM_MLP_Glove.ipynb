{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (part 1)\n",
    "\n",
    "Implementing second part of solution described here: \n",
    "\"Franck Dernoncourt, Ji Young Lee, and Peter\n",
    "Szolovits. 2016. Neural networks for joint sentence\n",
    "classification in medical paper abstracts. European\n",
    "Chapter of the Association for Computational Linguistics\n",
    "(EACL) 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \n",
       "0  OBJECTIVE  \n",
       "1    METHODS  \n",
       "2    METHODS  \n",
       "3    METHODS  \n",
       "4    METHODS  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file PubMed_20k_RCT.csv created by script01_create_single_dataset\n",
    "df_all = pd.read_csv('input/PubMed_20k_RCT.csv')\n",
    "df_train = df_all[df_all['partition']=='train']\n",
    "df_valid = df_all[df_all['partition']=='dev']\n",
    "df_test = df_all[df_all['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train partition size: 180040\n",
      "Valid partition size: 30212\n",
      "Test partition size: 30135\n",
      "Total dataset size: 240387\n"
     ]
    }
   ],
   "source": [
    "X_train_cnt = df_train.shape[0]\n",
    "X_valid_cnt = df_valid.shape[0]\n",
    "X_test_cnt = df_test.shape[0]\n",
    "\n",
    "X_all = df_all.text.values\n",
    "\n",
    "print('Train partition size: {}'.format(X_train_cnt))\n",
    "print('Valid partition size: {}'.format(X_valid_cnt))\n",
    "print('Test partition size: {}'.format(X_test_cnt))\n",
    "print('Total dataset size: {}'.format(X_all.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 67356\n",
      "CPU times: user 11.8 s, sys: 82.5 ms, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_all)\n",
    "sequences = tokenizer.texts_to_sequences(X_all)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "VOC_SIZE = len(word_index)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "X_token_seq_all = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='pre', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 58 ms, total: 1.9 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "labels = df_all.label.values\n",
    "label_dict = {label: no for no, label in enumerate(set(labels))}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare embedding layer with pre-trained glove word vectors\n",
    "\n",
    "I use glove pre-trained (on Twiter) embeddings. <br> \n",
    "My code follows guidelines from Keras tutorial: <br>\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding dim = 200\n",
      "Number of word vectors found: 1193514\n",
      "CPU times: user 1min 3s, sys: 2.08 s, total: 1min 5s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read pre-trained glove embeddings\n",
    "\n",
    "# create dictionary with word embeddings\n",
    "embedding_file = {}\n",
    "embedding_file[25] = 'glove/glove.twitter.27B.25d.txt'\n",
    "embedding_file[50] = 'glove/glove.twitter.27B.50d.txt'\n",
    "embedding_file[100] = 'glove/glove.twitter.27B.100d.txt'\n",
    "embedding_file[200] = 'glove/glove.twitter.27B.200d.txt'\n",
    "\n",
    "# set embedding dimension\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Using embedding dim = {}'.format(EMBEDDING_DIM))\n",
    "\n",
    "f = open(embedding_file[EMBEDDING_DIM], 'r')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Number of word vectors found: {}'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 30.7 ms, total: 153 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create matrix with embedding coefs for each word in vocabulary\n",
    "# row position of word representation in the matrix will be the word number\n",
    "\n",
    "# initiazes matrix with zeros\n",
    "embedding_matrix = np.zeros((VOC_SIZE + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 200)           13471400  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 800)               3203200   \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               80100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_2 (Dense)    (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 16,756,825\n",
      "Trainable params: 3,285,425\n",
      "Non-trainable params: 13,471,400\n",
      "_________________________________________________________________\n",
      "Train on 180040 samples, validate on 30212 samples\n",
      "Epoch 1/20\n",
      "180040/180040 [==============================] - 6242s 35ms/step - loss: 0.8771 - acc: 0.6656 - val_loss: 0.5649 - val_acc: 0.7916\n",
      "Epoch 2/20\n",
      "180040/180040 [==============================] - 2382s 13ms/step - loss: 0.6323 - acc: 0.7774 - val_loss: 0.4937 - val_acc: 0.8185\n",
      "Epoch 3/20\n",
      "180040/180040 [==============================] - 2407s 13ms/step - loss: 0.5782 - acc: 0.7987 - val_loss: 0.4737 - val_acc: 0.8309\n",
      "Epoch 4/20\n",
      "180040/180040 [==============================] - 2356s 13ms/step - loss: 0.5219 - acc: 0.8181 - val_loss: 0.4616 - val_acc: 0.8308\n",
      "Epoch 5/20\n",
      "180040/180040 [==============================] - 2535s 14ms/step - loss: 0.4874 - acc: 0.8308 - val_loss: 0.4507 - val_acc: 0.8369\n",
      "Epoch 6/20\n",
      "180040/180040 [==============================] - 2788s 15ms/step - loss: 0.4468 - acc: 0.8416 - val_loss: 0.4727 - val_acc: 0.8394\n",
      "CPU times: user 1d 6h 52min 36s, sys: 2h 37min 34s, total: 1d 9h 30min 11s\n",
      "Wall time: 5h 11min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_layer = Embedding(VOC_SIZE + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm = LSTM(units=800, return_sequences=False)(embedded_sequences)\n",
    "D1 = Dense(100, activation='relu', name='sentence_vector_1')(lstm)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "D1 = Dense(20, activation='relu', name='sentence_vector_2')(D1)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "preds = Dense(len(label_dict), activation='softmax')(D1)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# learn\n",
    "model.fit(X_token_seq_all[:X_train_cnt], y_all[:X_train_cnt], \\\n",
    "          validation_data=(X_token_seq_all[X_train_cnt:(X_train_cnt+X_valid_cnt)], \\\n",
    "                                     y_all[X_train_cnt:(X_train_cnt+X_valid_cnt)]), \\\n",
    "          callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "          verbose=1, epochs=20, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('input/Baseline_Part1_LSTM_MPL.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
