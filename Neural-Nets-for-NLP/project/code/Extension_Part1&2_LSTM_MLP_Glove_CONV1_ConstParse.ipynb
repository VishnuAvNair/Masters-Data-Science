{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension 2 - Adding Contituent Parse Tree tags to the input \n",
    "\n",
    "Implementing solution described in the paper below, using text + contituent parse tree tags as features: \n",
    "\"Franck Dernoncourt, Ji Young Lee, and Peter\n",
    "Szolovits. 2016. Neural networks for joint sentence\n",
    "classification in medical paper abstracts. European\n",
    "Chapter of the Association for Computational Linguistics\n",
    "(EACL) 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Tokens (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \n",
       "0  OBJECTIVE  \n",
       "1    METHODS  \n",
       "2    METHODS  \n",
       "3    METHODS  \n",
       "4    METHODS  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file PubMed_20k_RCT.csv created by script01_create_single_dataset\n",
    "df_all_text = pd.read_csv('input/PubMed_20k_RCT.csv')\n",
    "df_train_text = df_all_text[df_all_text['partition']=='train']\n",
    "df_valid_text = df_all_text[df_all_text['partition']=='dev']\n",
    "df_test_text = df_all_text[df_all_text['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train partition size: 180040\n",
      "Valid partition size: 30212\n",
      "Test partition size: 30135\n",
      "Total dataset size: 240387\n"
     ]
    }
   ],
   "source": [
    "X_train_cnt = df_train_text.shape[0]\n",
    "X_valid_cnt = df_valid_text.shape[0]\n",
    "X_test_cnt = df_test_text.shape[0]\n",
    "\n",
    "X_all_text = df_all_text.text.values\n",
    "\n",
    "print('Train partition size: {}'.format(X_train_cnt))\n",
    "print('Valid partition size: {}'.format(X_valid_cnt))\n",
    "print('Test partition size: {}'.format(X_test_cnt))\n",
    "print('Total dataset size: {}'.format(X_all_text.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Contituent Parse Tree tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>abstract_id</th>\n",
       "      <th>seq</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>deptree</th>\n",
       "      <th>deptree2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>0</td>\n",
       "      <td>To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>( ROOT ( S ( S ( VP ( TO ) ( VP ( VB ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( JJ ) ( JJ ) ( NN ) ) ) ) ) ) ( PP ( IN ) ( S ( VP ( VBG ) ( NP ( NP ( NN ) ) ( , ) ( NP ( NN ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( JJ ) ( NN ) ) ( PP ( IN ) ( NP ( DT ) ( JJ ) ( NN ) ) ) ) ) ) ) ) ) ) ) ( CC ) ( S ( NP ( IN ) ( DT ) ( NN ) ) ( VP ( MD ) ( VP ( VB ) ( VP ( VBN ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( NP ( JJR ) ( NNS ) ) ( PP ( IN ...</td>\n",
       "      <td>( ROOT ( S ( S ( VP ( TO ) ( VP ( VB ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( JJ ) ( JJ ) ( NN ) ) ) ) ) ) ( PP ( IN ) ( S ( VP ( VBG ) ( NP ( NP ( NN ) ) ( , ) ( NP ( NN ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( JJ ) ( NN ) ) ( PP ( IN ) ( NP ( DT ) ( JJ ) ( NN ) ) ) ) ) ) ) ) ) ) ) ( CC ) ( S ( NP ( IN ) ( DT ) ( NN ) ) ( VP ( MD ) ( VP ( VB ) ( VP ( VBN ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( NP ( JJR ) ( NNS ) ) ( PP ( IN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>1</td>\n",
       "      <td>A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>( ROOT ( S ( S ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( NN ) ( NN ) ) ) ) ) ) ( VP ( VBD ) ( VP ( VBN ) ( NP ( CD ) ) ) ) ) ( : ) ( S ( NP ( CD ) ) ( VP ( VBD ) ( SBAR ( S ( NP ( NP ( CD ) ( NN mg/day ) ) ( PP ( IN ) ( NP ( NP ( NN ) ) ( CC ) ( NP ( CD ) ) ) ) ) ( VP ( VBD ) ( NP ( NN ) ) ( PP ( IN ) ( NP ( CD ) ( NNS ) ) ) ) ) ) ) ) ( . ) ) )</td>\n",
       "      <td>( ROOT ( S ( S ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( NN ) ( NN ) ) ) ) ) ) ( VP ( VBD ) ( VP ( VBN ) ( NP ( CD ) ) ) ) ) ( ( S ( NP ( CD ) ) ( VP ( VBD ) ( SBAR ( S ( NP ( NP ( CD ) ( NN ) ( PP ( IN ) ( NP ( NP ( NN ) ) ( CC ) ( NP ( CD ) ) ) ) ) ( VP ( VBD ) ( NP ( NN ) ) ( PP ( IN ) ( NP ( CD ) ( NNS ) ) ) ) ) ) ) ) ( . ) ) )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>2</td>\n",
       "      <td>Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>( ROOT ( S ( NP ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NN ) ( NN ) ( CC ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( NN ) ( NNS ) ) ( CC ) ( NP ( JJ ) ( NN ) ( NNS ) ) ) ) ) ( . ) ) )</td>\n",
       "      <td>( ROOT ( S ( NP ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NN ) ( NN ) ( CC ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( NN ) ( NNS ) ) ( CC ) ( NP ( JJ ) ( NN ) ( NNS ) ) ) ) ) ( . ) ) )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>3</td>\n",
       "      <td>Pain was assessed using the visual analog pain scale ( 0-100 mm ) .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>( ROOT ( S ( NP ( NN ) ) ( VP ( VBD ) ( VP ( VBN ) ( S ( VP ( VBG ) ( NP ( NP ( DT ) ( JJ ) ( NN ) ( NN ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( CD ) ( NN ) ) ( -RRB- ) ) ) ) ) ) ) ( . ) ) )</td>\n",
       "      <td>( ROOT ( S ( NP ( NN ) ) ( VP ( VBD ) ( VP ( VBN ) ( S ( VP ( VBG ) ( NP ( NP ( DT ) ( JJ ) ( NN ) ( NN ) ( NN ) ) ( PRN ( LRB ) ( NP ( CD ) ( NN ) ) ( RRB ) ) ) ) ) ) ) ( . ) ) )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>4293578</td>\n",
       "      <td>4</td>\n",
       "      <td>Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>( ROOT ( S ( NP ( JJ ) ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NP ( NP ( DT ) ( JJ ) ( NNP ) ( CC ) ( NNP ) ( NNS ) ) ( NP ( NNP ) ( NNP ) ( NNS ) ) ) ( , ) ( NP ( NP ( NP ( NN ) ( JJ ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( NN ) ) ( -RRB- ) ) ) ( PP ( IN ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NN ) ( NN ) ) ) ) ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( NN ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( NN ) ) ( -RRB- ) ) ) ) ) ( . ) ) )</td>\n",
       "      <td>( ROOT ( S ( NP ( JJ ) ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NP ( NP ( DT ) ( JJ ) ( NNP ) ( CC ) ( NNP ) ( NNS ) ) ( NP ( NNP ) ( NNP ) ( NNS ) ) ) ( , ) ( NP ( NP ( NP ( NN ) ( JJ ) ( NN ) ) ( PRN ( LRB ) ( NP ( NN ) ) ( RRB ) ) ) ( PP ( IN ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NN ) ( NN ) ) ) ) ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( NN ) ( NN ) ) ( PRN ( LRB ) ( NP ( NN ) ) ( RRB ) ) ) ) ) ( . ) ) )</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  partition  abstract_id  seq  \\\n",
       "0     train      4293578    0   \n",
       "1     train      4293578    1   \n",
       "2     train      4293578    2   \n",
       "3     train      4293578    3   \n",
       "4     train      4293578    4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         text  \\\n",
       "0  To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .   \n",
       "1                                                                                                                                             A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .   \n",
       "2                                                                                                                                                                             Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .   \n",
       "3                                                                                                                                                                                                                         Pain was assessed using the visual analog pain scale ( 0-100 mm ) .   \n",
       "4                                                                           Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and 6-min walk distance ( 6MWD ) .   \n",
       "\n",
       "       label  \\\n",
       "0  OBJECTIVE   \n",
       "1    METHODS   \n",
       "2    METHODS   \n",
       "3    METHODS   \n",
       "4    METHODS   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               deptree  \\\n",
       "0  ( ROOT ( S ( S ( VP ( TO ) ( VP ( VB ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( JJ ) ( JJ ) ( NN ) ) ) ) ) ) ( PP ( IN ) ( S ( VP ( VBG ) ( NP ( NP ( NN ) ) ( , ) ( NP ( NN ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( JJ ) ( NN ) ) ( PP ( IN ) ( NP ( DT ) ( JJ ) ( NN ) ) ) ) ) ) ) ) ) ) ) ( CC ) ( S ( NP ( IN ) ( DT ) ( NN ) ) ( VP ( MD ) ( VP ( VB ) ( VP ( VBN ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( NP ( JJR ) ( NNS ) ) ( PP ( IN ...   \n",
       "1                                                                                                          ( ROOT ( S ( S ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( NN ) ( NN ) ) ) ) ) ) ( VP ( VBD ) ( VP ( VBN ) ( NP ( CD ) ) ) ) ) ( : ) ( S ( NP ( CD ) ) ( VP ( VBD ) ( SBAR ( S ( NP ( NP ( CD ) ( NN mg/day ) ) ( PP ( IN ) ( NP ( NP ( NN ) ) ( CC ) ( NP ( CD ) ) ) ) ) ( VP ( VBD ) ( NP ( NN ) ) ( PP ( IN ) ( NP ( CD ) ( NNS ) ) ) ) ) ) ) ) ( . ) ) )   \n",
       "2                                                                                                                                                                                                                                                                                                                                          ( ROOT ( S ( NP ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NN ) ( NN ) ( CC ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( NN ) ( NNS ) ) ( CC ) ( NP ( JJ ) ( NN ) ( NNS ) ) ) ) ) ( . ) ) )   \n",
       "3                                                                                                                                                                                                                                                                                                                              ( ROOT ( S ( NP ( NN ) ) ( VP ( VBD ) ( VP ( VBN ) ( S ( VP ( VBG ) ( NP ( NP ( DT ) ( JJ ) ( NN ) ( NN ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( CD ) ( NN ) ) ( -RRB- ) ) ) ) ) ) ) ( . ) ) )   \n",
       "4                                                                                   ( ROOT ( S ( NP ( JJ ) ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NP ( NP ( DT ) ( JJ ) ( NNP ) ( CC ) ( NNP ) ( NNS ) ) ( NP ( NNP ) ( NNP ) ( NNS ) ) ) ( , ) ( NP ( NP ( NP ( NN ) ( JJ ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( NN ) ) ( -RRB- ) ) ) ( PP ( IN ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NN ) ( NN ) ) ) ) ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( NN ) ( NN ) ) ( PRN ( -LRB- ) ( NP ( NN ) ) ( -RRB- ) ) ) ) ) ( . ) ) )   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              deptree2  \n",
       "0  ( ROOT ( S ( S ( VP ( TO ) ( VP ( VB ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( JJ ) ( JJ ) ( NN ) ) ) ) ) ) ( PP ( IN ) ( S ( VP ( VBG ) ( NP ( NP ( NN ) ) ( , ) ( NP ( NN ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( JJ ) ( NN ) ) ( PP ( IN ) ( NP ( DT ) ( JJ ) ( NN ) ) ) ) ) ) ) ) ) ) ) ( CC ) ( S ( NP ( IN ) ( DT ) ( NN ) ) ( VP ( MD ) ( VP ( VB ) ( VP ( VBN ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( NP ( JJR ) ( NNS ) ) ( PP ( IN ...  \n",
       "1                                                                                                                       ( ROOT ( S ( S ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( CD ) ( NNS ) ) ( PP ( IN ) ( NP ( JJ ) ( NN ) ( NN ) ) ) ) ) ) ( VP ( VBD ) ( VP ( VBN ) ( NP ( CD ) ) ) ) ) ( ( S ( NP ( CD ) ) ( VP ( VBD ) ( SBAR ( S ( NP ( NP ( CD ) ( NN ) ( PP ( IN ) ( NP ( NP ( NN ) ) ( CC ) ( NP ( CD ) ) ) ) ) ( VP ( VBD ) ( NP ( NN ) ) ( PP ( IN ) ( NP ( CD ) ( NNS ) ) ) ) ) ) ) ) ( . ) ) )  \n",
       "2                                                                                                                                                                                                                                                                                                                                          ( ROOT ( S ( NP ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NN ) ( NN ) ( CC ) ( NN ) ) ( PP ( IN ) ( NP ( NP ( NN ) ( NNS ) ) ( CC ) ( NP ( JJ ) ( NN ) ( NNS ) ) ) ) ) ( . ) ) )  \n",
       "3                                                                                                                                                                                                                                                                                                                                  ( ROOT ( S ( NP ( NN ) ) ( VP ( VBD ) ( VP ( VBN ) ( S ( VP ( VBG ) ( NP ( NP ( DT ) ( JJ ) ( NN ) ( NN ) ( NN ) ) ( PRN ( LRB ) ( NP ( CD ) ( NN ) ) ( RRB ) ) ) ) ) ) ) ( . ) ) )  \n",
       "4                                                                                           ( ROOT ( S ( NP ( JJ ) ( NN ) ( NNS ) ) ( VP ( VBD ) ( NP ( NP ( NP ( DT ) ( JJ ) ( NNP ) ( CC ) ( NNP ) ( NNS ) ) ( NP ( NNP ) ( NNP ) ( NNS ) ) ) ( , ) ( NP ( NP ( NP ( NN ) ( JJ ) ( NN ) ) ( PRN ( LRB ) ( NP ( NN ) ) ( RRB ) ) ) ( PP ( IN ) ( NP ( NP ( DT ) ( NN ) ) ( PP ( IN ) ( NP ( NN ) ( NN ) ) ) ) ) ) ( , ) ( CC ) ( NP ( NP ( JJ ) ( NN ) ( NN ) ) ( PRN ( LRB ) ( NP ( NN ) ) ( RRB ) ) ) ) ) ( . ) ) )  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_constree = pd.read_csv('input/PubMed_20k_RCT_CONSTPARSE.csv')\n",
    "df_train_constree = df_all_constree[df_all_constree['partition']=='train']\n",
    "df_valid_constree = df_all_constree[df_all_constree['partition']=='dev']\n",
    "df_test_constree = df_all_constree[df_all_constree['partition']=='test']\n",
    "pd.set_option('max_colwidth',500)\n",
    "df_all_constree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cnt2 = df_train_constree.shape[0]\n",
    "X_valid_cnt2 = df_valid_constree.shape[0]\n",
    "X_test_cnt2 = df_test_constree.shape[0]\n",
    "\n",
    "X_all_constree = df_all_constree.deptree2.values\n",
    "\n",
    "assert X_train_cnt2 == X_train_cnt\n",
    "assert X_valid_cnt2 == X_valid_cnt\n",
    "assert X_test_cnt2 == X_test_cnt\n",
    "assert X_all_text.shape[0] == X_all_constree.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Create Vector Representation (dim=100) for Text and POS TAG Sentences\n",
    "\n",
    "I will obtain a vectorized representation of dim = 100 of each sentence in it's text form and it's pos tag list form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Get Vector Representation for Text\n",
    "### 2.1.1 - Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 67356\n",
      "CPU times: user 22.7 s, sys: 311 ms, total: 23 s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer_text = Tokenizer()\n",
    "tokenizer_text.fit_on_texts(X_all_text)\n",
    "sequences_text = tokenizer_text.texts_to_sequences(X_all_text)\n",
    "\n",
    "word_index_text = tokenizer_text.word_index\n",
    "VOC_SIZE_text = len(word_index_text)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 50)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN_text = 50\n",
    "X_seq_all_text = pad_sequences(sequences_text, maxlen=MAX_SEQ_LEN_text, dtype='int32', \\\n",
    "                               padding='pre', truncating='post', value=0)\n",
    "print(X_seq_all_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Get vector represetation of sentence text using LSMT MLP model\n",
    "\n",
    "#### 2.1.2.1 Retrieve pre-trained LSMT MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 50, 200)           13471400  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 800)               3203200   \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               80100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_2 (Dense)    (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 16,756,825\n",
      "Trainable params: 3,285,425\n",
      "Non-trainable params: 13,471,400\n",
      "_________________________________________________________________\n",
      "CPU times: user 10 s, sys: 735 ms, total: 10.8 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read model that creates vector representation of sentences\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_text = load_model('input/Baseline_Part1_LSTM_MPL.h5')\n",
    "\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 Get vector with dim = 100 representing sentence text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 100)\n",
      "(30212, 100)\n",
      "(30135, 100)\n",
      "(240387, 100)\n",
      "CPU times: user 2h 32min 45s, sys: 3min 22s, total: 2h 36min 8s\n",
      "Wall time: 48min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model_text.input, outputs=model_text.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train_text = intermediate_layer_model.predict(X_seq_all_text[:X_train_cnt])\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid_text = intermediate_layer_model.predict(X_seq_all_text[X_train_cnt:(X_train_cnt+X_valid_cnt)])\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test_text = intermediate_layer_model.predict(X_seq_all_text[-X_test_cnt:])\n",
    "\n",
    "# concat all data\n",
    "sentence_all_text = np.vstack((sentence_train_text, sentence_valid_text, sentence_test_text))\n",
    "\n",
    "print(sentence_train_text.shape)\n",
    "print(sentence_valid_text.shape)\n",
    "print(sentence_test_text.shape)\n",
    "print(sentence_all_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Get Vector Representation for Constituent Tree\n",
    "### 2.2.1 - Tokenize Constituent Tree tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 50\n",
      "CPU times: user 32.3 s, sys: 2.44 s, total: 34.7 s\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# will only use 50 most common tokens\n",
    "\n",
    "all_text = ' '.join(X_all_constree)\n",
    "all_tokens = [token for token in all_text.split()]\n",
    "top_tokens = Counter(all_tokens).most_common(50)\n",
    "i = 0\n",
    "word_index = {k:i+1 for i, (k, _) in enumerate(top_tokens)}\n",
    "\n",
    "VOC_SIZE_constree = len(word_index)\n",
    "print('Vocabulary size = {}'.format(VOC_SIZE_constree))\n",
    "\n",
    "sequences_constree = [[word_index[token] for token in parsetree.split() if word_index.get(token,-1)!=-1] \\\n",
    "                      for parsetree in X_all_constree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 300)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN_constree = 300\n",
    "X_seq_all_constree = pad_sequences(sequences_constree, maxlen=MAX_SEQ_LEN_constree, dtype='int32', \\\n",
    "                               padding='pre', truncating='post', value=0)\n",
    "print(X_seq_all_constree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - One hot encode Constituent Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 2, 2, 2],\n",
       "       [0, 0, 0, ..., 2, 2, 2],\n",
       "       [0, 0, 0, ..., 2, 2, 2],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 2, 2, 2],\n",
       "       [0, 0, 0, ..., 2, 2, 2],\n",
       "       [0, 0, 0, ..., 2, 2, 2]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq_all_constree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 15000)\n",
      "(240387, 300, 50)\n",
      "CPU times: user 13min 3s, sys: 24.9 s, total: 13min 28s\n",
      "Wall time: 21min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_DIM_constree = VOC_SIZE_constree\n",
    "\n",
    "def get_vector(token):\n",
    "    vector = np.zeros(EMBEDDING_DIM_constree)\n",
    "    if token != 0:\n",
    "        vector[token-1] = 1\n",
    "    return vector\n",
    "\n",
    "X_seq_all_constree_v = np.zeros((X_seq_all_constree.shape[0], X_seq_all_constree.shape[1] * EMBEDDING_DIM_constree))\n",
    "print(X_seq_all_constree_v.shape)\n",
    "i = 0\n",
    "for token_seq in X_seq_all_constree:\n",
    "    X_seq_all_constree_v[i] = np.array([get_vector(token) for token in token_seq]).flatten()\n",
    "    i += 1\n",
    "\n",
    "X_seq_all_constree_v = X_seq_all_constree_v.reshape(X_seq_all_constree_v.shape[0], \\\n",
    "                                               int(X_seq_all_constree_v.shape[1]/EMBEDDING_DIM_constree), \\\n",
    "                                               EMBEDDING_DIM_constree)\n",
    "print(X_seq_all_constree_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Get vector represetation of Constituent Tree text using pre-trained ConvD1 model\n",
    "\n",
    "#### 2.2.3.1 Retrieve pre-trained Constituent Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 296, 64)           16064     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 292, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 58, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 54, 128)           41088     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 50, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 173,149\n",
      "Trainable params: 173,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 4.34 s, sys: 701 ms, total: 5.04 s\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read model that creates vector representation of sentences\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_constree = load_model('input/Extension_Part1_CONV1_CONSTPARSE1.h5')\n",
    "\n",
    "model_constree.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.2 Get vector with dim = 100 representing sentence POS TAG sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 100)\n",
      "(30212, 100)\n",
      "(30135, 100)\n",
      "(240387, 100)\n",
      "CPU times: user 16min 47s, sys: 1min 44s, total: 18min 31s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model_constree.input, outputs=model_constree.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train_constree = intermediate_layer_model.predict(X_seq_all_constree_v[:X_train_cnt])\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid_constree = intermediate_layer_model.predict(X_seq_all_constree_v[X_train_cnt:(X_train_cnt+X_valid_cnt)])\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test_constree = intermediate_layer_model.predict(X_seq_all_constree_v[-X_test_cnt:])\n",
    "\n",
    "# concat all data\n",
    "sentence_all_constree = np.vstack((sentence_train_constree, sentence_valid_constree, sentence_test_constree))\n",
    "\n",
    "print(sentence_train_constree.shape)\n",
    "print(sentence_valid_constree.shape)\n",
    "print(sentence_test_constree.shape)\n",
    "print(sentence_all_constree.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Vectorize output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 5)\n",
      "CPU times: user 5.05 s, sys: 783 ms, total: 5.84 s\n",
      "Wall time: 9.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "labels = df_all_text.label.values\n",
    "label_dict = {label: no for no, label in enumerate(set(labels))}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])\n",
    "\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Combine Text and Constituent Tree representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 200)\n",
      "(30212, 200)\n",
      "(30135, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train_comb = np.hstack((sentence_train_text, sentence_train_constree))\n",
    "X_valid_comb = np.hstack((sentence_valid_text, sentence_valid_constree))\n",
    "X_test_comb = np.hstack((sentence_test_text, sentence_test_constree))\n",
    "print(X_train_comb.shape)\n",
    "print(X_valid_comb.shape)\n",
    "print(X_test_comb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Train combined model (text and Constituent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "sentence_vector_1 (Dense)    (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 62,425\n",
      "Trainable params: 62,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 180040 samples, validate on 30212 samples\n",
      "Epoch 1/20\n",
      "180040/180040 [==============================] - 20s 111us/step - loss: 1.1323 - acc: 0.8394 - val_loss: 1.1365 - val_acc: 0.8458\n",
      "Epoch 2/20\n",
      "180040/180040 [==============================] - 18s 99us/step - loss: 1.0505 - acc: 0.8608 - val_loss: 1.1505 - val_acc: 0.8438\n",
      "CPU times: user 51.9 s, sys: 5.02 s, total: 56.9 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "comb = Input(shape=(200,), dtype='float32')\n",
    "D1 = Dense(units=200, activation='relu')(comb)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "D1 = Dense(units=100, activation='relu', name='sentence_vector_1')(D1)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "D1 = Dense(units=20, activation='relu')(D1)\n",
    "D1 = Dropout(0.5)(D1)\n",
    "out = Dense(5,\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "            activity_regularizer=regularizers.l1(0.01),\n",
    "            activation=\"softmax\")(D1) \n",
    "\n",
    "model = Model(comb, out)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# learn\n",
    "model.fit(X_train_comb, y_all[:X_train_cnt], \\\n",
    "          validation_data=(X_valid_comb, y_all[X_train_cnt:(X_train_cnt+X_valid_cnt)]), \\\n",
    "          callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "          verbose=1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6 - Save combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('input/Extension_Part1_LSTM_MLP_Glove_CONV1_constree.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create sentence and label sequences\n",
    "\n",
    "### 7.1 - Find max length of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of abstracts = 20000\n",
      "Max abstract size = 31\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAD8CAYAAAAG/FfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEnqMf8fOwQBSugfdSwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187785f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find size of abstracts\n",
    "\n",
    "# create dataframe with abstract lengths\n",
    "abstract_lens = df_all_text[['abstract_id','text']].groupby(by='abstract_id').agg('count')\n",
    "\n",
    "# create dict with abstract lengths\n",
    "abstract_len_dict = {abstract_id: length for abstract_id, length in abstract_lens.reset_index().values}\n",
    "\n",
    "print('Total number of abstracts = {}'.format(len(abstract_lens.index)))\n",
    "print('Max abstract size = {}'.format(np.max(abstract_lens.text.values)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "axes[0].boxplot(abstract_lens.text.values)\n",
    "axes[1].hist(abstract_lens.text.values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Redo vectorization of output labels - add 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240387, 6)\n",
      "CPU times: user 4.43 s, sys: 92.7 ms, total: 4.52 s\n",
      "Wall time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_labels = df_all_text.label.values\n",
    "\n",
    "# create label set and add 'PAD' to the set\n",
    "label_set = set(y_labels)\n",
    "label_set.add('PAD')\n",
    "label_dict = {label: no for no, label in enumerate(label_set)}\n",
    "number_of_classes = len(label_dict)\n",
    "\n",
    "# get inverted dict\n",
    "label_dict_inv = {no: label for label, no in label_dict.items()}\n",
    "\n",
    "# get labels as integers\n",
    "y_all = [label_dict[label] for label in y_labels]\n",
    "\n",
    "# change y to categorical (vectorize output)\n",
    "y_all = np.array([to_categorical(i, num_classes=number_of_classes) for i in y_all])\n",
    "\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 - Get vectorized representation of sentence text + POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 100)\n",
      "(30212, 100)\n",
      "(30135, 100)\n",
      "(240387, 100)\n",
      "CPU times: user 12.4 s, sys: 1.43 s, total: 13.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get tensor representation of sentences as output from layer \"sentence_vector_2\"\n",
    "\n",
    "layer_name = 'sentence_vector_1'\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "# get tensor representation of training partition\n",
    "sentence_train = intermediate_layer_model.predict(X_train_comb)\n",
    "\n",
    "# get tensor representation of validation partition\n",
    "sentence_valid = intermediate_layer_model.predict(X_valid_comb)\n",
    "\n",
    "# get tensor representation of test partition\n",
    "sentence_test = intermediate_layer_model.predict(X_test_comb)\n",
    "\n",
    "# concat all data\n",
    "sentence_all = np.vstack((sentence_train, sentence_valid, sentence_test))\n",
    "\n",
    "print(sentence_train.shape)\n",
    "print(sentence_valid.shape)\n",
    "print(sentence_test.shape)\n",
    "print(sentence_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 - Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Number of abstracts: 20000\n",
      "Number of label seqs: 20000\n",
      "\n",
      "Number of abstracts in each partition: \n",
      "partition\n",
      "dev       2500\n",
      "test      2500\n",
      "train    15000\n",
      "Name: abstract_id, dtype: int64\n",
      "CPU times: user 2.38 s, sys: 361 ms, total: 2.74 s\n",
      "Wall time: 3.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get abstract and label sequences\n",
    "\n",
    "def get_abstract_seq(sentence_arry, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    sentences = sentence_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.zeros(100 * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_abstract = np.hstack((left_padding, sentences))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_abstract) == 3100\n",
    "    return full_abstract\n",
    "\n",
    "def get_label_seq(label_arry, pad, i, j):\n",
    "    # create flattened array with sentences for this abstract\n",
    "    labels = label_arry[i:j].flatten()\n",
    "    # create enough zero padding to reach 31 sentences, the max no of sentences for abstract in dataset\n",
    "    left_padding  = np.array(list(pad) * (31 - (j - i)))\n",
    "    # pad left\n",
    "    full_labels = np.hstack((left_padding, labels))\n",
    "    # abstract must be 31 sentences long, and each sentence has len 100\n",
    "    assert len(full_labels) == 31 * 6\n",
    "    return full_labels\n",
    "\n",
    "# create lists to store abstracts and label sequence for abstracts\n",
    "abstracts = list()\n",
    "labels = list()\n",
    "sorted_abs_id = list()\n",
    "\n",
    "# find categorial \n",
    "y_PAD = to_categorical(label_dict['PAD'], num_classes=number_of_classes)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    \n",
    "    # find number of sentences in abstract\n",
    "    abs_id = df_all_text.abstract_id.values[i]\n",
    "    # save district abstract ids, in the order they appear in the dataset\n",
    "    sorted_abs_id.append(abs_id)\n",
    "    abs_len = abstract_len_dict[abs_id]\n",
    "    j = i + abs_len\n",
    "    \n",
    "    # get flattened sentences that make up abstract\n",
    "    abstracts.append(get_abstract_seq(sentence_all, i, j))\n",
    "    \n",
    "    # get flattened labels that make up abstract labels\n",
    "    labels.append(get_label_seq(y_all, y_PAD, i, j))\n",
    "    \n",
    "    i = j\n",
    "    if j >= len(df_all_text.index):\n",
    "        print('Done!')\n",
    "        break\n",
    "        \n",
    "print('Number of abstracts: {}'.format(len(abstracts)))\n",
    "print('Number of label seqs: {}'.format(len(labels)))\n",
    "\n",
    "print('\\nNumber of abstracts in each partition: \\n{}'.format(df_all_text.groupby(by=('partition')).abstract_id.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 31, 100)\n",
      "(2500, 31, 100)\n",
      "(2500, 31, 100)\n",
      "(15000, 31, 6)\n",
      "(2500, 31, 6)\n",
      "(2500, 31, 6)\n"
     ]
    }
   ],
   "source": [
    "# restore partition data\n",
    "\n",
    "X_all = np.array(abstracts)\n",
    "X_all = X_all.reshape(X_all.shape[0],31,100)\n",
    "X_train = X_all[:15000]\n",
    "X_valid = X_all[15000:17500]\n",
    "X_test  = X_all[-2500:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "y_seq_all = np.array(labels)\n",
    "y_seq_all = y_seq_all.reshape(y_seq_all.shape[0],31,6)\n",
    "y_seq_train = y_seq_all[:15000]\n",
    "y_seq_valid = y_seq_all[15000:17500]\n",
    "y_seq_test  = y_seq_all[-2500:]\n",
    "\n",
    "print(y_seq_train.shape)\n",
    "print(y_seq_valid.shape)\n",
    "print(y_seq_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 31, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 31, 200)           160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 31, 6)             1206      \n",
      "=================================================================\n",
      "Total params: 162,006\n",
      "Trainable params: 162,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15000 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.1293 - acc: 0.9639 - val_loss: 0.1034 - val_acc: 0.9625\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 25s 2ms/step - loss: 0.0749 - acc: 0.9703 - val_loss: 0.0983 - val_acc: 0.9631\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 24s 2ms/step - loss: 0.0719 - acc: 0.9713 - val_loss: 0.0963 - val_acc: 0.9636\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 27s 2ms/step - loss: 0.0701 - acc: 0.9719 - val_loss: 0.0951 - val_acc: 0.9647\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 28s 2ms/step - loss: 0.0691 - acc: 0.9721 - val_loss: 0.0932 - val_acc: 0.9649\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 26s 2ms/step - loss: 0.0679 - acc: 0.9725 - val_loss: 0.0942 - val_acc: 0.9659\n",
      "CPU times: user 8min 45s, sys: 30.2 s, total: 9min 16s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Embedding, Input, LSTM, Flatten, Dropout, Dense, TimeDistributed, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "abs = Input(shape=(X_all.shape[1], X_all.shape[2]), dtype='float32')\n",
    "lstm = Bidirectional(LSTM(units=100, return_sequences=True))(abs)\n",
    "out = TimeDistributed(Dense(6, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model_final = Model(abs, out)\n",
    "model_final.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_final.summary()\n",
    "\n",
    "# learn\n",
    "model_final.fit(X_train, y_seq_train, validation_data=(X_valid, y_seq_valid), \\\n",
    "            callbacks=[EarlyStopping(patience=1, monitor='val_loss')], \\\n",
    "            verbose=1, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final.save('input/Extension_final_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Final prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test partition (padded label sequence): 0.9635225806451613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = model_final.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1).flatten()\n",
    "\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1).flatten()\n",
    "\n",
    "acc = accuracy_score(y_seq_true, y_seq_hat)\n",
    "print('Accuracy on test partition (padded label sequence): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test partition (no padding): 0.9061888169902107\n"
     ]
    }
   ],
   "source": [
    "preds = model_final.predict(X_test)\n",
    "y_seq_hat = np.argmax(preds, axis=-1)\n",
    "y_seq_true = np.argmax(y_seq_test, axis=-1)\n",
    "\n",
    "#print(y_seq_hat.shape)\n",
    "#print(y_seq_true.shape)\n",
    "\n",
    "y_seq_hat_trimmed = list()\n",
    "y_seq_true_trimmed = list()\n",
    "i = 0\n",
    "for abs_id in sorted_abs_id[-2500:]:\n",
    "    x = list(y_seq_hat[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_hat_trimmed.insert(0, x.pop())\n",
    "    x = list(y_seq_true[i][-abstract_len_dict[abs_id]:])\n",
    "    while len(x) > 0:\n",
    "        y_seq_true_trimmed.insert(0, x.pop())    \n",
    "    i += 1\n",
    "    \n",
    "#print(len(y_seq_hat_trimmed))\n",
    "#print(len(y_seq_true_trimmed))\n",
    "\n",
    "acc = accuracy_score(y_seq_true_trimmed, y_seq_hat_trimmed)\n",
    "print('Accuracy on test partition (no padding): {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9061888169902107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_seq_true_trimmed, y_seq_hat_trimmed, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
