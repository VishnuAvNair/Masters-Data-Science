# Neural Networks for Natural Language Processing

This course focused on how to apply machine learning and particularly deep learning algorithms to solve problems in Computational Linguistics and Natural Language Processing.  Among other topics, the course covered word vector representations, window-based neural networks, recurrent neural nets, long-short-term-memory models, recursive neural nets, and CNN. 

## Syllabus

* Intro do Computational Linguistics and NLP    
* NLTK, WordNet, spaCy, TensorFlow, Keras    
* Vectorization and Word2vec    
* Word Window Classification and Neural Networks    
* Backpropagation, gradients, overfitting, activation function    
* Recurrent Neural Networks and Language Models    
* Gated Feedback Recurrent NNs, Long Short-Term Memory for Machine Translation    
* Recursive Neural Networks, Parsing, and Sentiment Analysis    
* Convolutional Neural Networks, Sentence Classification    
* General topics: ML, Speech Recognition, Dynamic Memory Networks    

Full syllabus [here]()

## Course Project

[Project Report]()    
[Project Presentation]()       
[Project Presentation Youtube Video]()       


## Assignments

[Assignment 1 - Stanford CoreNLP, spaCy, fastText]()    
[Assignment 2 - Part of speach tagging with NNs and Dialog Act classification]()    
[Assignment 3 - Text classification and sentence level classification with CNNs and RNNs]()    
[Assignment 4 - Text classification and sentence level classification with LSTM Networks]()    
[Assignment 5 - Neural Nets in NLP Competitions - Two Recent Examples from Kaggle]()       
[Assignemnt 5 Youtube Video]()


