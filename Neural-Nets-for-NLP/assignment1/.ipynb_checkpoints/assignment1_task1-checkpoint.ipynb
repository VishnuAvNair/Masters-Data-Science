{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 1 - Task 1 \n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read toxic comments dataset and create train and test partitions\n",
    "\n",
    "Source: Kaggle Toxic Comment Classification Challenge (https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('input/train.csv')\n",
    "drop_cols = ['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "df_all = df_all.drop(drop_cols, axis=1)\n",
    "#df_all = df_all.sample(frac=0.2)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of toxic comments: 0.09584448302009764\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of toxic comments: {}'.format(df_all['toxic'].sum() / df_all['toxic'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize text\n",
    "import re\n",
    "pat = re.compile(u'[^a-zA-Z0-9]')\n",
    "def normalize(txt):\n",
    "    return pat.sub(' ',txt)\n",
    "    \n",
    "df_all['comment_text'] = df_all['comment_text'].apply(lambda x: normalize(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train, test partitions\n",
    "X_all = df_all.comment_text.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, df_all.toxic.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create benchmark using BOW (and GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 4873)\n",
      "(47872, 4873)\n",
      "CPU times: user 1min 24s, sys: 2.93 s, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract BOW as tfidf sparce matrix\n",
    "vectorizer = TfidfVectorizer(\\\n",
    "                             ngram_range=(1,3),\n",
    "                             stop_words='english',\n",
    "                             min_df=0.001,\n",
    "                             max_df=0.99,\n",
    "                             sublinear_tf=True\n",
    "                            )\n",
    "vectorizer.fit(X_all)\n",
    "X_train_csr = vectorizer.transform(X_train)\n",
    "X_test_csr = vectorizer.transform(X_test)\n",
    "print(X_train_csr.shape)\n",
    "print(X_test_csr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9548379010695187\n",
      "CPU times: user 1.63 s, sys: 88 ms, total: 1.72 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train and predict\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_csr, y_train)\n",
    "y_hat = clf.predict(X_test_csr)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize POS tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "{'': 1, 'JJ': 2, 'JJR': 3, 'NNS': 4, 'DT': 5, 'HYPH': 6, 'RBR': 7, 'AFX': 8, 'PRP': 9, 'FW': 10, 'IN': 11, 'RB': 12, 'NNP': 13, 'VBN': 14, '-RRB-': 15, 'VB': 16, 'TO': 17, 'WDT': 18, 'CC': 19, 'EX': 20, 'XX': 21, 'VBZ': 22, 'WRB': 23, '``': 24, 'UH': 25, 'PRP$': 26, 'SYM': 27, 'LS': 28, 'WP': 29, 'POS': 30, 'VBG': 31, 'PDT': 32, 'WP$': 33, 'MD': 34, 'JJS': 35, 'NN': 36, '$': 37, 'VBD': 38, 'VBP': 39, 'NNPS': 40, 'ADD': 41, 'CD': 42, 'RBS': 43, 'RP': 44, '_SP': 45}\n",
      "CPU times: user 1min 7s, sys: 3.06 s, total: 1min 10s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionary with pos tags - will use 1000 rows of data\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "SIZE=1000\n",
    "\n",
    "def doc_generator():\n",
    "    for txt in X_all[:SIZE]:\n",
    "        yield nlp(txt)\n",
    " \n",
    "doc_generator = doc_generator()\n",
    "tag_voc = dict()\n",
    "for doc in doc_generator:\n",
    "    for token in doc:\n",
    "        tag_voc[token.tag_] = 1\n",
    "\n",
    "idx = 0\n",
    "for k, v in tag_voc.items():\n",
    "    idx += 1\n",
    "    tag_voc[k] = idx\n",
    "    \n",
    "print(len(tag_voc))\n",
    "print(tag_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 52min 13s, sys: 5min 16s, total: 1h 57min 29s\n",
      "Wall time: 29min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create list of vectors for train partition\n",
    "\n",
    "def train_vec_generator():\n",
    "    for txt in X_train:\n",
    "        doc = nlp(txt)\n",
    "        yield [tag_voc.get(token.tag_,999) for token in doc]\n",
    "\n",
    "vec_generator = train_vec_generator()\n",
    "max_len = 0\n",
    "train_vec_list = list()\n",
    "for vec in vec_generator:\n",
    "    train_vec_list.append(vec)\n",
    "    vec_len = len(vec)\n",
    "    if vec_len > max_len:\n",
    "        max_len = vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47min 34s, sys: 2min 9s, total: 49min 43s\n",
      "Wall time: 12min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create list of vectors for test partition with proper length (pad with zeros to match comment with longest length)\n",
    "\n",
    "def test_vec_generator():\n",
    "    for txt in X_test:\n",
    "        doc = nlp(txt)\n",
    "        yield [tag_voc.get(token.tag_,999) for token in doc]\n",
    "\n",
    "vec_generator = test_vec_generator()\n",
    "test_vec_list = list()\n",
    "for vec in vec_generator:\n",
    "    test_vec_list.append(vec)\n",
    "    vec_len = len(vec)\n",
    "    if vec_len > max_len:\n",
    "        max_len = vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 s, sys: 2.29 s, total: 27.4 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create train pos tag vector with proper length (pad with zeros to match comment with longest length)\n",
    "i=0\n",
    "for vec in train_vec_list:\n",
    "    train_vec_list[i] = [0] * (max_len - len(vec)) + vec\n",
    "    i += 1\n",
    "    \n",
    "X_train_pos_tag = np.array(train_vec_list)\n",
    "del train_vec_list\n",
    "gc.collect()\n",
    "\n",
    "# create train pos tag vector with proper length\n",
    "i=0\n",
    "for vec in test_vec_list:\n",
    "    test_vec_list[i] = [0] * (max_len - len(vec)) + vec\n",
    "    i += 1\n",
    "    \n",
    "X_test_pos_tag = np.array(test_vec_list)\n",
    "del test_vec_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier on BOW + POS tag (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 6958)\n",
      "(47872, 6958)\n",
      "CPU times: user 3.35 s, sys: 1.21 s, total: 4.56 s\n",
      "Wall time: 4.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_train_csr_2 = hstack((X_train_csr, X_train_pos_tag.reshape(X_train_pos_tag.shape[0], max_len))).tocsr()\n",
    "X_test_csr_2 = hstack((X_test_csr, X_test_pos_tag.reshape(X_test_pos_tag.shape[0], max_len))).tocsr()\n",
    "print(X_train_csr_2.shape)\n",
    "print(X_test_csr_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9504512032085561\n",
      "CPU times: user 1min 45s, sys: 2.75 s, total: 1min 48s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# BOW + NLP features\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n",
    "clf.fit(X_train_csr_2, y_train)\n",
    "y_hat = clf.predict(X_test_csr_2)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9490307486631016\n",
      "CPU times: user 42.2 s, sys: 795 ms, total: 43 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# BOW\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n",
    "clf.fit(X_train_csr, y_train)\n",
    "y_hat = clf.predict(X_test_csr)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
