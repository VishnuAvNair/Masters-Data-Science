{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 2 - Task 1 \n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/carlos2/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/carlos2/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 610 ms, total: 13.7 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MAX_SEQ = 200\n",
    "\n",
    "# create lists of tokens and tags from corpora\n",
    "# add UNK to map unknown tag values, PAD to pad sequences for RNN\n",
    "tokens, tags = zip(*brown.tagged_words(tagset='universal'))\n",
    "tokens = list(tokens)\n",
    "tokens.append('PAD')\n",
    "tags = list(tags)\n",
    "tags.append('PAD')\n",
    "\n",
    "# create token embedding dictionary and inverse dictionary\n",
    "token_dict = {token: no for no, token in enumerate(set(tokens))}\n",
    "token_dict_inv = {no: token for token, no in token_dict.items()}\n",
    "\n",
    "# create tag embedding dictionary and inverse dictionary\n",
    "tag_dict = {tag: no for no, tag in enumerate(set(tags))}\n",
    "tag_dict_inv = {no: tag for tag, no in tag_dict.items()}\n",
    "\n",
    "# Create dictionary with sentence number as key, \n",
    "# and tuple with lists of token, token numbers, tags, tag numbers  \n",
    "\n",
    "sents = {}\n",
    "start = 0\n",
    "for sent_no, token_list in enumerate(brown.sents()):\n",
    "    end = start + len(token_list)\n",
    "    sents[sent_no] = {}\n",
    "    # get tags for this sentence, and then embedding\n",
    "    tokens    = tokens[start: end]\n",
    "    token_nos = [token_dict[token] for token in tokens]\n",
    "    tags      = tags[start: end]\n",
    "    tag_nos   = [tag_dict[tag] for tag in tags]\n",
    "    sents[sent_no] = (tokens, token_nos, tags, tag_nos)\n",
    "    start = end\n",
    "    \n",
    "# pad token and tag sequences to MAX_SEQ with PAD to get X and y for sequence to sequence model\n",
    "\n",
    "token_seqs = [sents[1][1] for sents in sents.items()]\n",
    "X = pad_sequences(maxlen=MAX_SEQ, sequences=token_seqs, padding=\"pre\", value=token_dict['PAD'])\n",
    "\n",
    "tag_seqs = [sents[1][3] for sents in sents.items()]\n",
    "y = pad_sequences(maxlen=MAX_SEQ, sequences=tag_seqs, padding=\"pre\", value=tag_dict['PAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500 21500\n",
      " 21500 21500 21500 21500 21500 21500 21500 14914 55304 25049  3012 55666\n",
      " 37539 42710 17041  8347 26354  6218 53518 55021 27110 52859  5369 29457\n",
      " 35890 33218 44370 12030 31261 33811   178  6365]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 7 4 4 2 4 8 4 7 4 1\n",
      " 4 2 4 4 8 5 7 4 5 1 7 4 8 4 5]\n"
     ]
    }
   ],
   "source": [
    "# check if X and y were properly created\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM bidirectional seq to seq model\n",
    "# will vectorize word embeddings to vectors of size wordvec_dim\n",
    "\n",
    "WORDVEC_DIM = 20\n",
    "\n",
    "def get_model(max_seq, voc_size, wordvec_dim, tag_count):\n",
    "    \n",
    "    input = Input(shape=(max_seq,), dtype='float64')\n",
    "    embed = Embedding(input_dim=voc_size, output_dim=wordvec_dim, input_length=MAX_SEQ, \\\n",
    "                      embeddings_initializer='uniform')(input)\n",
    "    lstm = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(embed)\n",
    "    out = TimeDistributed(Dense(tag_count, activation=\"softmax\"))(lstm) \n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(token_dict)\n",
    "number_of_classes = len(tag_dict)\n",
    "\n",
    "# need to change y to categorical \n",
    "ycat = [to_categorical(i, num_classes=number_of_classes) for i in y]\n",
    "\n",
    "# reserve 10% of the dataset for testing after training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ycat, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 20)           1121160   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 200)          96800     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 200, 13)           2613      \n",
      "=================================================================\n",
      "Total params: 1,220,573\n",
      "Trainable params: 1,220,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40138 samples, validate on 17202 samples\n",
      "Epoch 1/2\n",
      "40138/40138 [==============================] - 300s 7ms/step - loss: 0.0075 - acc: 0.9992 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      " 7360/40138 [====>.........................] - ETA: 3:44 - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2020a9d1ce60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWORDVEC_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#plt.plot(history.history['loss'], label='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_model(MAX_SEQ, vocabulary_size, WORDVEC_DIM, number_of_classes)\n",
    "\n",
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=2, validation_split=0.3, verbose=1)\n",
    "\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')   \n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "acc = accuracy_score(np.argmax(np.array(y_test), axis=1), np.argmax(y_hat, axis=1))\n",
    "print('Accuracy on test partition: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_train, axis=-1)[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.array(y_test), axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(ycat[0][-25:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, ycat, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500, 21500,\n",
       "       21500, 21500, 21500, 21500, 14914, 55304, 25049,  3012, 55666,\n",
       "       37539, 42710, 17041,  8347, 26354,  6218, 53518, 55021, 27110,\n",
       "       52859,  5369, 29457, 35890, 33218, 44370, 12030, 31261, 33811,\n",
       "         178,  6365], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57340, 200)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 200)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
