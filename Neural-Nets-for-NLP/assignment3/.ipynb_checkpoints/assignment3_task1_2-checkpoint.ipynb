{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 3 - Task 1.2\n",
    "\n",
    "Sentence Classification based on Kim, Yoon paper \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014) <br>\n",
    "Model variation: CNN-rand\n",
    "\n",
    "Dataset used: MR - Movie Reviews <br>\n",
    "Reference: Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics, 2005\n",
    "\n",
    "In this notebook we add NLP POS tag features to Kim, Yoon model to try to improve results.\n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10662\n",
      "Count of positive reviews: 5331\n",
      "Count of negative reviews: 5331\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>the sentimental cliches mar an otherwise excel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>if you love the music , and i do , its hard to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>though harris is affecting at times , he canno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>poignant japanese epic about adolescent anomie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>cantet perfectly captures the hotel lobbies , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating\n",
       "1837  the sentimental cliches mar an otherwise excel...       1\n",
       "3318  if you love the music , and i do , its hard to...       1\n",
       "3381  though harris is affecting at times , he canno...       0\n",
       "3387  poignant japanese epic about adolescent anomie...       1\n",
       "36    cantet perfectly captures the hotel lobbies , ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "df_neg = pd.read_table('input/rt-polarity.neg', names=['review'],  header=None, encoding='latin-1')\n",
    "df_pos = pd.read_table('input/rt-polarity.pos', names=['review'],  header=None, encoding='latin-1')\n",
    "df_neg['rating'] = 0\n",
    "df_pos['rating'] = 1\n",
    "df_all = shuffle(pd.concat((df_neg, df_pos), axis=0), random_state=SEED)\n",
    "print('Dataset size: {}'.format(df_all.index.size))\n",
    "print('Count of positive reviews: {}'.format(df_all[df_all['rating']==1].index.size))\n",
    "print('Count of negative reviews: {}'.format(df_all[df_all['rating']==0].index.size))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences for text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# following guidelines outlined here:\n",
    "# https://keras.io/preprocessing/text/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/text.py#L134\n",
    "# https://keras.io/preprocessing/sequence/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/sequence.py#L248\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize text and create dictionary mapping tokens to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_all.review)\n",
    "\n",
    "# create sequences of integers to represent reviews, and find longest sentence\n",
    "seqs = tokenizer.texts_to_sequences(df_all.review)\n",
    "max_len = max([len(seq) for seq in seqs])\n",
    "\n",
    "# pad sequences to feed to the embedding layer\n",
    "seqs = pad_sequences(seqs, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents  = 10662\n",
      "Size of vocabulary   = 19498\n",
      "Maximum sequence len = 51\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents  = {}'.format(tokenizer.document_count))\n",
    "print('Size of vocabulary   = {}'.format(len(tokenizer.word_index)))\n",
    "print('Maximum sequence len = {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences for token POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tag arry shape: (10662,)\n",
      "CPU times: user 6min 16s, sys: 8.89 s, total: 6min 25s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# create list to track tags for each review\n",
    "tag_list = list()\n",
    "tag_idx_list = list()\n",
    "\n",
    "# create dictionaries to track vacabulary indexes\n",
    "voc_tag = dict()\n",
    "voc_tag_cnt = 0\n",
    "\n",
    "for review in df_all.review.tolist():\n",
    "    comm_tag_list = list()\n",
    "    comm_tag_idx_list = list()\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        w = token.text\n",
    "        if str.strip(w) not in ['','.']:      \n",
    "            # tag int (fine pos tag)\n",
    "            g = token.tag_\n",
    "            if voc_tag.get(g,voc_tag_cnt) == voc_tag_cnt:\n",
    "                voc_tag[g] = voc_tag_cnt \n",
    "                voc_tag_cnt += 1\n",
    "            comm_tag_list.append(g)           \n",
    "            comm_tag_idx_list.append(voc_tag[g]) \n",
    "    tag_list.append(comm_tag_list)\n",
    "    tag_idx_list.append(comm_tag_idx_list)\n",
    "\n",
    "X_tag = np.array(tag_list)\n",
    "X_tag_idx = np.array(tag_idx_list)\n",
    "\n",
    "print('X_tag arry shape: {}'.format(X_tag.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences to feed to the embedding layer\n",
    "tag_seqs = pad_sequences(X_tag_idx, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "I will use \"ConvNet Architectures\" guidelines from Stanford \"CS231n: Convolutional Neural Networks for Visual Recognition\" class guidelines: http://cs231n.github.io/convolutional-networks/ <br>\n",
    "\"INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC <br>\n",
    "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)...\"\n",
    "\n",
    "The model will have 2 branches that will be merged on axis = 1, then feed to a final 1 deep fully connect MLP layer\n",
    "Branch 1 will be for token sequences; embedding later will created vectorized representation for tokens\n",
    "Branch 2 will be for tag sequence and tags; I don't created vectorized representations for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_67 (InputLayer)           (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_67 (Embedding)        (None, 51, 64)       1247936     input_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_68 (Embedding)        (None, 51, 32)       1600        input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 51, 128)      1572992     embedding_67[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 51, 64)       196672      embedding_68[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_185 (Dropout)           (None, 51, 128)      0           conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_187 (Dropout)           (None, 51, 64)       0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling1D) (None, 26, 128)      0           dropout_185[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling1D) (None, 26, 64)       0           dropout_187[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_66 (Flatten)            (None, 3328)         0           max_pooling1d_66[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_67 (Flatten)            (None, 1664)         0           max_pooling1d_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_152 (Dense)               (None, 16)           53264       flatten_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_153 (Dense)               (None, 1)            1665        flatten_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_186 (Dropout)           (None, 16)           0           dense_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_188 (Dropout)           (None, 1)            0           dense_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 17)           0           dropout_186[0][0]                \n",
      "                                                                 dropout_188[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_154 (Dense)               (None, 16)           288         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_189 (Dropout)           (None, 16)           0           dense_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_155 (Dense)               (None, 1)            17          dropout_189[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,074,434\n",
      "Trainable params: 3,074,434\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = max_len\n",
    "VOC_SIZE1 = len(tokenizer.word_index)\n",
    "VOC_SIZE2 = len(voc_tag)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "# model where N=1, M=1 and K=1\n",
    "def get_model(output_dim=64, filter_size=128, window_size=3, stride=1, pool_size=2, dense_dim=16):\n",
    "    \n",
    "    # Branch 1\n",
    "    krn_size = output_dim * window_size\n",
    "    input1 = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "    embed1 = Embedding(VOC_SIZE1+1, output_dim, input_length=MAX_SEQ, embeddings_initializer='random_uniform')(input1)\n",
    "    C1 = Conv1D(filter_size, kernel_size=krn_size ,padding='same', strides=(stride), activation='relu')(embed1)\n",
    "    C1 = Dropout(0.5)(C1)\n",
    "    M1 = MaxPooling1D(pool_size=(pool_size), padding='same')(C1)\n",
    "    F1 = Flatten()(M1)\n",
    "    D1 = Dense(dense_dim, activation='relu')(F1)\n",
    "    D1 = Dropout(0.5)(D1)\n",
    "    \n",
    "    # Branch2\n",
    "    input2 = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "    embed2 = Embedding(VOC_SIZE2+1, int(output_dim/2), input_length=MAX_SEQ, \\\n",
    "                       embeddings_initializer='random_uniform')(input2)\n",
    "    C2 = Conv1D(int(filter_size/2), kernel_size=int(krn_size/2) ,padding='same', \\\n",
    "                       strides=(stride), activation='relu')(embed2)\n",
    "    C2 = Dropout(0.5)(C2)\n",
    "    M2 = MaxPooling1D(pool_size=(pool_size), padding='same')(C2)\n",
    "    F2 = Flatten()(M2)\n",
    "    D2 = Dense(1, activation='relu')(F2)\n",
    "    D2 = Dropout(0.5)(D2)\n",
    "    \n",
    "    # Merge branches by concatenating along axis 1\n",
    "    L1 = concatenate([D1, D2], axis=1)\n",
    "    G1= Dense(dense_dim, activation='relu')(L1)\n",
    "    G1 = Dropout(0.5)(G1)\n",
    "\n",
    "    pred = Dense(1, kernel_regularizer=regularizers.l2(0.01),\\\n",
    "                 activity_regularizer=regularizers.l1(0.01),\\\n",
    "                 activation='sigmoid')(G1)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9594 samples, validate on 1068 samples\n",
      "Epoch 1/20\n",
      " - 86s - loss: 1.2332 - acc: 0.4992 - val_loss: 1.1683 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 85s - loss: 1.0973 - acc: 0.4993 - val_loss: 1.0188 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 86s - loss: 0.9128 - acc: 0.6161 - val_loss: 1.0176 - val_acc: 0.7238\n",
      "Epoch 4/20\n",
      " - 82s - loss: 0.8052 - acc: 0.8039 - val_loss: 1.1089 - val_acc: 0.7425\n",
      "\n",
      "\t>> Score for split 1: 0.7425093632958801\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 87s - loss: 1.2308 - acc: 0.4970 - val_loss: 1.1857 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 85s - loss: 1.0794 - acc: 0.4998 - val_loss: 1.0548 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 85s - loss: 0.8865 - acc: 0.5509 - val_loss: 1.0759 - val_acc: 0.5685\n",
      "\n",
      "\t>> Score for split 2: 0.5684803001876173\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 83s - loss: 1.2529 - acc: 0.5046 - val_loss: 1.1942 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 84s - loss: 1.1175 - acc: 0.5010 - val_loss: 1.0539 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 82s - loss: 0.9252 - acc: 0.6838 - val_loss: 1.0333 - val_acc: 0.6698\n",
      "Epoch 4/20\n",
      " - 84s - loss: 0.7982 - acc: 0.7942 - val_loss: 1.1021 - val_acc: 0.7233\n",
      "\n",
      "\t>> Score for split 3: 0.723264540337711\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 87s - loss: 1.2295 - acc: 0.4981 - val_loss: 1.1798 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 91s - loss: 1.0767 - acc: 0.4991 - val_loss: 1.0574 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 82s - loss: 0.9192 - acc: 0.5793 - val_loss: 1.0415 - val_acc: 0.5947\n",
      "Epoch 4/20\n",
      " - 92s - loss: 0.8269 - acc: 0.7589 - val_loss: 1.0596 - val_acc: 0.7336\n",
      "\n",
      "\t>> Score for split 4: 0.7335834896810507\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 87s - loss: 1.2712 - acc: 0.5052 - val_loss: 1.2353 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 90s - loss: 1.1721 - acc: 0.4965 - val_loss: 1.1266 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 85s - loss: 1.0074 - acc: 0.7025 - val_loss: 1.0306 - val_acc: 0.6989\n",
      "Epoch 4/20\n",
      " - 84s - loss: 0.8765 - acc: 0.8437 - val_loss: 1.0425 - val_acc: 0.7486\n",
      "\n",
      "\t>> Score for split 5: 0.7485928705440901\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 88s - loss: 1.2279 - acc: 0.4986 - val_loss: 1.1444 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 89s - loss: 1.0641 - acc: 0.4976 - val_loss: 1.0446 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 82s - loss: 0.8808 - acc: 0.6514 - val_loss: 1.0703 - val_acc: 0.7627\n",
      "\n",
      "\t>> Score for split 6: 0.7626641651031895\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 90s - loss: 1.2623 - acc: 0.5010 - val_loss: 1.2246 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 83s - loss: 1.1575 - acc: 0.5049 - val_loss: 1.0421 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 84s - loss: 0.9687 - acc: 0.6977 - val_loss: 1.0050 - val_acc: 0.6463\n",
      "Epoch 4/20\n",
      " - 87s - loss: 0.8421 - acc: 0.8049 - val_loss: 1.0334 - val_acc: 0.7214\n",
      "\n",
      "\t>> Score for split 7: 0.7213883677298312\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 85s - loss: 1.2642 - acc: 0.4965 - val_loss: 1.1904 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 86s - loss: 1.1368 - acc: 0.4951 - val_loss: 1.0689 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 85s - loss: 0.9938 - acc: 0.5519 - val_loss: 1.0146 - val_acc: 0.6304\n",
      "Epoch 4/20\n",
      " - 87s - loss: 0.8904 - acc: 0.7634 - val_loss: 1.0501 - val_acc: 0.7139\n",
      "\n",
      "\t>> Score for split 8: 0.7138836772983115\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 85s - loss: 1.2320 - acc: 0.4965 - val_loss: 1.2124 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 88s - loss: 1.0993 - acc: 0.5053 - val_loss: 1.0464 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 91s - loss: 0.9062 - acc: 0.6830 - val_loss: 1.0362 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      " - 87s - loss: 0.8018 - acc: 0.8146 - val_loss: 1.0398 - val_acc: 0.7439\n",
      "\n",
      "\t>> Score for split 9: 0.7439024390243902\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 87s - loss: 1.2898 - acc: 0.5050 - val_loss: 1.2309 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 88s - loss: 1.2009 - acc: 0.4998 - val_loss: 1.1155 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 88s - loss: 1.0711 - acc: 0.6296 - val_loss: 1.0318 - val_acc: 0.5394\n",
      "Epoch 4/20\n",
      " - 82s - loss: 0.9300 - acc: 0.7475 - val_loss: 1.0056 - val_acc: 0.6745\n",
      "Epoch 5/20\n",
      " - 88s - loss: 0.8363 - acc: 0.8025 - val_loss: 1.0385 - val_acc: 0.7552\n",
      "\n",
      "\t>> Score for split 10: 0.7551594746716698\n",
      "\n",
      "Average accuracy = 0.7213428687873742\n",
      "CPU times: user 5h 42min 44s, sys: 16min, total: 5h 58min 45s\n",
      "Wall time: 56min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# evaluate using 10-fold CV as in Yoon Kim article\n",
    "FOLDS = 10\n",
    "\n",
    "# concatenates input data so kfold splits all data as one\n",
    "X = np.hstack((seqs, tag_seqs))\n",
    "y = np.array(df_all.rating.tolist())\n",
    "kfold = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "acc = list()\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X, y):\n",
    "    i += 1\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    model = get_model()\n",
    "    # divide input back into seqs and tag_seqs\n",
    "    model.fit([X_train[:,:MAX_SEQ], X_train[:,MAX_SEQ:]], y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, \\\n",
    "              validation_data=([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]], y_valid),\\\n",
    "              callbacks=[EarlyStopping(patience=1, monitor='val_loss')])\n",
    "    y_hat = model.predict([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]])\n",
    "    y_pred = [round(pred) for pred in y_hat.reshape(-1)]\n",
    "    acc.append(accuracy_score(y_valid, y_pred))\n",
    "    print('\\n\\t>> Score for split {}: {}\\n'.format(i, acc[-1]))\n",
    "\n",
    "print('Average accuracy = {}'.format(np.mean(np.array(acc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
