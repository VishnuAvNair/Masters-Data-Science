{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 3 - Task 2.3\n",
    "\n",
    "Sentence Classification with recurrent neural net (LSTM)\n",
    "I will compare result with results for CCN-Rand reported in paper by Kim, Yoon entitled \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014) <br>\n",
    "\n",
    "In this notebook, I add POS tag to the feature set used for the classification task, to compare results with and without the addition o \n",
    "\n",
    "Dataset used: MR - Movie Reviews <br>\n",
    "Reference: Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics, 2005\n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10662\n",
      "Count of positive reviews: 5331\n",
      "Count of negative reviews: 5331\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>the sentimental cliches mar an otherwise excel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>if you love the music , and i do , its hard to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>though harris is affecting at times , he canno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>poignant japanese epic about adolescent anomie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>cantet perfectly captures the hotel lobbies , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating\n",
       "1837  the sentimental cliches mar an otherwise excel...       1\n",
       "3318  if you love the music , and i do , its hard to...       1\n",
       "3381  though harris is affecting at times , he canno...       0\n",
       "3387  poignant japanese epic about adolescent anomie...       1\n",
       "36    cantet perfectly captures the hotel lobbies , ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "df_neg = pd.read_table('input/rt-polarity.neg', names=['review'],  header=None, encoding='latin-1')\n",
    "df_pos = pd.read_table('input/rt-polarity.pos', names=['review'],  header=None, encoding='latin-1')\n",
    "df_neg['rating'] = 0\n",
    "df_pos['rating'] = 1\n",
    "df_all = shuffle(pd.concat((df_neg, df_pos), axis=0), random_state=SEED)\n",
    "print('Dataset size: {}'.format(df_all.index.size))\n",
    "print('Count of positive reviews: {}'.format(df_all[df_all['rating']==1].index.size))\n",
    "print('Count of negative reviews: {}'.format(df_all[df_all['rating']==0].index.size))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences for text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# following guidelines outlined here:\n",
    "# https://keras.io/preprocessing/text/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/text.py#L134\n",
    "# https://keras.io/preprocessing/sequence/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/sequence.py#L248\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize text and create dictionary mapping tokens to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_all.review)\n",
    "\n",
    "# create sequences of integers to represent reviews, and find longest sentence\n",
    "seqs = tokenizer.texts_to_sequences(df_all.review)\n",
    "max_len = max([len(seq) for seq in seqs])\n",
    "\n",
    "# pad sequences to feed to the embedding layer\n",
    "seqs = pad_sequences(seqs, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents  = 10662\n",
      "Size of vocabulary   = 19498\n",
      "Maximum sequence len = 51\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents  = {}'.format(tokenizer.document_count))\n",
    "print('Size of vocabulary   = {}'.format(len(tokenizer.word_index)))\n",
    "print('Maximum sequence len = {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences for token POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tag arry shape: (10662,)\n",
      "CPU times: user 5min 56s, sys: 8.22 s, total: 6min 5s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# create list to track tags for each review\n",
    "tag_list = list()\n",
    "tag_idx_list = list()\n",
    "\n",
    "# create dictionaries to track vacabulary indexes\n",
    "voc_tag = dict()\n",
    "voc_tag_cnt = 0\n",
    "\n",
    "for review in df_all.review.tolist():\n",
    "    comm_tag_list = list()\n",
    "    comm_tag_idx_list = list()\n",
    "    doc = nlp(review)\n",
    "    for token in doc:\n",
    "        w = token.text\n",
    "        if str.strip(w) not in ['','.']:      \n",
    "            # tag int (fine pos tag)\n",
    "            g = token.tag_\n",
    "            if voc_tag.get(g,voc_tag_cnt) == voc_tag_cnt:\n",
    "                voc_tag[g] = voc_tag_cnt \n",
    "                voc_tag_cnt += 1\n",
    "            comm_tag_list.append(g)           \n",
    "            comm_tag_idx_list.append(voc_tag[g]) \n",
    "    tag_list.append(comm_tag_list)\n",
    "    tag_idx_list.append(comm_tag_idx_list)\n",
    "\n",
    "X_tag = np.array(tag_list)\n",
    "X_tag_idx = np.array(tag_idx_list)\n",
    "\n",
    "print('X_tag arry shape: {}'.format(X_tag.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sequences to feed to the embedding layer\n",
    "tag_seqs = pad_sequences(X_tag_idx, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "The model will have 2 branches that will be merged on axis = 1, then fed to a final 1 deep fully connect MLP layer\n",
    "\n",
    "Branch 1 will be for LSTM using embedding layer with vectorized representation of tokens <br>\n",
    "Branch 2 will be for CNN using embedding layer with vectorized representation of tags <br>\n",
    "\n",
    "For branch 2 I will use \"ConvNet Architectures\" guidelines from Stanford \"CS231n: Convolutional Neural Networks for Visual Recognition\" class guidelines: http://cs231n.github.io/convolutional-networks/ <br>\n",
    "\"INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC <br>\n",
    "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)...\"\n",
    "\n",
    "In this case N=1, M=1 and K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 51, 32)       1600        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 51, 64)       196672      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 51, 64)       1247936     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 51, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 51, 200)      132000      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 26, 64)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10200)        0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1664)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           163216      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1665        flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17)           0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           288         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            17          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,743,394\n",
      "Trainable params: 1,743,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = max_len\n",
    "VOC_SIZE1 = len(tokenizer.word_index)\n",
    "VOC_SIZE2 = len(voc_tag)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras import regularizers\n",
    "\n",
    "def get_model(output_dim=64, filter_size=128, window_size=3, stride=1, pool_size=2, dense_dim=16):\n",
    "    \n",
    "    # Branch 1\n",
    "    input1 = Input(shape=(MAX_SEQ,), dtype='float64')\n",
    "    embed1 = Embedding(VOC_SIZE1+1, output_dim, input_length=MAX_SEQ, embeddings_initializer='random_uniform')(input1)\n",
    "    lstm = Bidirectional(LSTM(units=100, return_sequences=True))(embed1)\n",
    "    lstm = Flatten()(lstm)\n",
    "    D1 = Dense(dense_dim, activation='relu')(lstm)\n",
    "    D1 = Dropout(0.5)(D1)\n",
    "    \n",
    "    # Branch2\n",
    "    krn_size = output_dim * window_size\n",
    "    input2 = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "    embed2 = Embedding(VOC_SIZE2+1, int(output_dim/2), input_length=MAX_SEQ, \\\n",
    "                       embeddings_initializer='random_uniform')(input2)\n",
    "    C2 = Conv1D(int(filter_size/2), kernel_size=int(krn_size/2) ,padding='same', \\\n",
    "                       strides=(stride), activation='relu')(embed2)\n",
    "    C2 = Dropout(0.5)(C2)\n",
    "    M2 = MaxPooling1D(pool_size=(pool_size), padding='same')(C2)\n",
    "    F2 = Flatten()(M2)\n",
    "    D2 = Dense(1, activation='relu')(F2)\n",
    "    D2 = Dropout(0.5)(D2)\n",
    "    \n",
    "    # Merge branches by concatenating along axis 1\n",
    "    L1 = concatenate([D1, D2], axis=1)\n",
    "    G1= Dense(dense_dim, activation='relu')(L1)\n",
    "    G1 = Dropout(0.5)(G1)\n",
    "\n",
    "    pred = Dense(1, kernel_regularizer=regularizers.l2(0.01),\\\n",
    "                 activity_regularizer=regularizers.l1(0.01),\\\n",
    "                 activation='sigmoid')(G1)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9594 samples, validate on 1068 samples\n",
      "Epoch 1/20\n",
      " - 24s - loss: 1.1920 - acc: 0.4995 - val_loss: 1.1200 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 23s - loss: 0.9951 - acc: 0.4998 - val_loss: 1.0198 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 24s - loss: 0.8571 - acc: 0.5001 - val_loss: 1.1205 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      " - 23s - loss: 0.7829 - acc: 0.6262 - val_loss: 1.2411 - val_acc: 0.7566\n",
      "\n",
      "\t>> Score for split 1: 0.7565543071161048\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 25s - loss: 1.2774 - acc: 0.5038 - val_loss: 1.1632 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 21s - loss: 1.1219 - acc: 0.5842 - val_loss: 1.0335 - val_acc: 0.6764\n",
      "Epoch 3/20\n",
      " - 22s - loss: 0.9656 - acc: 0.7609 - val_loss: 1.0120 - val_acc: 0.6970\n",
      "Epoch 4/20\n",
      " - 23s - loss: 0.8592 - acc: 0.8319 - val_loss: 1.0948 - val_acc: 0.7392\n",
      "Epoch 5/20\n",
      " - 21s - loss: 0.8128 - acc: 0.8814 - val_loss: 1.1326 - val_acc: 0.7505\n",
      "\n",
      "\t>> Score for split 2: 0.7504690431519699\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 24s - loss: 1.2563 - acc: 0.5045 - val_loss: 1.1220 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 21s - loss: 1.0823 - acc: 0.5968 - val_loss: 1.0074 - val_acc: 0.6379\n",
      "Epoch 3/20\n",
      " - 21s - loss: 0.9113 - acc: 0.7624 - val_loss: 1.0514 - val_acc: 0.7101\n",
      "Epoch 4/20\n",
      " - 23s - loss: 0.8053 - acc: 0.8497 - val_loss: 1.1037 - val_acc: 0.7214\n",
      "\n",
      "\t>> Score for split 3: 0.7213883677298312\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 25s - loss: 1.2013 - acc: 0.4967 - val_loss: 1.0673 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 22s - loss: 1.0037 - acc: 0.5466 - val_loss: 1.0542 - val_acc: 0.6220\n",
      "Epoch 3/20\n",
      " - 21s - loss: 0.8635 - acc: 0.6704 - val_loss: 1.2051 - val_acc: 0.6914\n",
      "Epoch 4/20\n",
      " - 21s - loss: 0.7823 - acc: 0.7547 - val_loss: 1.2934 - val_acc: 0.7148\n",
      "\n",
      "\t>> Score for split 4: 0.7148217636022514\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 26s - loss: 1.2842 - acc: 0.4948 - val_loss: 1.2004 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 22s - loss: 1.1466 - acc: 0.5180 - val_loss: 1.0461 - val_acc: 0.5141\n",
      "Epoch 3/20\n",
      " - 22s - loss: 0.9608 - acc: 0.6927 - val_loss: 1.0611 - val_acc: 0.6876\n",
      "Epoch 4/20\n",
      " - 21s - loss: 0.8346 - acc: 0.7987 - val_loss: 1.1821 - val_acc: 0.7411\n",
      "\n",
      "\t>> Score for split 5: 0.7410881801125704\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 26s - loss: 1.2136 - acc: 0.4993 - val_loss: 1.0811 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 23s - loss: 1.0103 - acc: 0.5352 - val_loss: 0.9997 - val_acc: 0.6257\n",
      "Epoch 3/20\n",
      " - 22s - loss: 0.8719 - acc: 0.6571 - val_loss: 1.0328 - val_acc: 0.7364\n",
      "Epoch 4/20\n",
      " - 23s - loss: 0.7942 - acc: 0.7618 - val_loss: 1.1648 - val_acc: 0.7589\n",
      "\n",
      "\t>> Score for split 6: 0.7589118198874296\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 26s - loss: 1.2291 - acc: 0.5011 - val_loss: 1.1265 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 22s - loss: 1.0552 - acc: 0.5721 - val_loss: 1.0053 - val_acc: 0.6304\n",
      "Epoch 3/20\n",
      " - 23s - loss: 0.8750 - acc: 0.7444 - val_loss: 1.0465 - val_acc: 0.7542\n",
      "Epoch 4/20\n",
      " - 25s - loss: 0.7854 - acc: 0.8276 - val_loss: 1.1297 - val_acc: 0.7645\n",
      "\n",
      "\t>> Score for split 7: 0.7645403377110694\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 27s - loss: 1.2605 - acc: 0.4947 - val_loss: 1.0923 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 23s - loss: 1.0874 - acc: 0.6147 - val_loss: 1.0321 - val_acc: 0.6745\n",
      "Epoch 3/20\n",
      " - 23s - loss: 0.9320 - acc: 0.7402 - val_loss: 1.0894 - val_acc: 0.7195\n",
      "Epoch 4/20\n",
      " - 23s - loss: 0.8146 - acc: 0.8353 - val_loss: 1.2006 - val_acc: 0.7486\n",
      "\n",
      "\t>> Score for split 8: 0.7485928705440901\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 25s - loss: 1.2824 - acc: 0.4985 - val_loss: 1.1962 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 21s - loss: 1.1713 - acc: 0.5502 - val_loss: 1.0720 - val_acc: 0.6792\n",
      "Epoch 3/20\n",
      " - 21s - loss: 1.0450 - acc: 0.7060 - val_loss: 1.0687 - val_acc: 0.7364\n",
      "Epoch 4/20\n",
      " - 22s - loss: 0.9185 - acc: 0.8129 - val_loss: 1.0878 - val_acc: 0.6942\n",
      "Epoch 5/20\n",
      " - 24s - loss: 0.8201 - acc: 0.8684 - val_loss: 1.1538 - val_acc: 0.7167\n",
      "\n",
      "\t>> Score for split 9: 0.7166979362101313\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 28s - loss: 1.2415 - acc: 0.5011 - val_loss: 1.0794 - val_acc: 0.5075\n",
      "Epoch 2/20\n",
      " - 22s - loss: 1.0752 - acc: 0.6085 - val_loss: 1.0272 - val_acc: 0.7195\n",
      "Epoch 3/20\n",
      " - 23s - loss: 0.9250 - acc: 0.7702 - val_loss: 1.0370 - val_acc: 0.6567\n",
      "Epoch 4/20\n",
      " - 22s - loss: 0.8237 - acc: 0.8423 - val_loss: 1.1248 - val_acc: 0.7317\n",
      "\n",
      "\t>> Score for split 10: 0.7317073170731707\n",
      "\n",
      "Average accuracy = 0.7404771943138619\n",
      "CPU times: user 1h 39min 45s, sys: 9min 21s, total: 1h 49min 7s\n",
      "Wall time: 17min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# evaluate using 10-fold CV as in Yoon Kim article\n",
    "FOLDS = 10\n",
    "\n",
    "# concatenates input data so kfold splits all data as one\n",
    "X = np.hstack((seqs, tag_seqs))\n",
    "y = np.array(df_all.rating.tolist())\n",
    "kfold = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "acc = list()\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X, y):\n",
    "    i += 1\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    model = get_model()\n",
    "    # divide input back into seqs and tag_seqs\n",
    "    model.fit([X_train[:,:MAX_SEQ], X_train[:,MAX_SEQ:]], y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, \\\n",
    "              validation_data=([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]], y_valid),\\\n",
    "              callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    y_hat = model.predict([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]])\n",
    "    y_pred = [round(pred) for pred in y_hat.reshape(-1)]\n",
    "    acc.append(accuracy_score(y_valid, y_pred))\n",
    "    print('\\n\\t>> Score for split {}: {}\\n'.format(i, acc[-1]))\n",
    "\n",
    "print('Average accuracy = {}'.format(np.mean(np.array(acc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
