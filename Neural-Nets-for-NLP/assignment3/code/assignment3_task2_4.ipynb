{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 3 - Task 2.4\n",
    "\n",
    "Sentence Classification with recurrent neural net (LSTM)\n",
    "I will compare result with results for CCN-Rand reported in paper by Kim, Yoon entitled \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014) <br>\n",
    "\n",
    "In this notebook, I combine LSTM with CNN for the classification task\n",
    "\n",
    "Dataset used: MR - Movie Reviews <br>\n",
    "Reference: Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics, 2005\n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10662\n",
      "Count of positive reviews: 5331\n",
      "Count of negative reviews: 5331\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>the sentimental cliches mar an otherwise excel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>if you love the music , and i do , its hard to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>though harris is affecting at times , he canno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>poignant japanese epic about adolescent anomie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>cantet perfectly captures the hotel lobbies , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating\n",
       "1837  the sentimental cliches mar an otherwise excel...       1\n",
       "3318  if you love the music , and i do , its hard to...       1\n",
       "3381  though harris is affecting at times , he canno...       0\n",
       "3387  poignant japanese epic about adolescent anomie...       1\n",
       "36    cantet perfectly captures the hotel lobbies , ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "df_neg = pd.read_table('input/rt-polarity.neg', names=['review'],  header=None, encoding='latin-1')\n",
    "df_pos = pd.read_table('input/rt-polarity.pos', names=['review'],  header=None, encoding='latin-1')\n",
    "df_neg['rating'] = 0\n",
    "df_pos['rating'] = 1\n",
    "df_all = shuffle(pd.concat((df_neg, df_pos), axis=0), random_state=SEED)\n",
    "print('Dataset size: {}'.format(df_all.index.size))\n",
    "print('Count of positive reviews: {}'.format(df_all[df_all['rating']==1].index.size))\n",
    "print('Count of negative reviews: {}'.format(df_all[df_all['rating']==0].index.size))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences for text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# following guidelines outlined here:\n",
    "# https://keras.io/preprocessing/text/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/text.py#L134\n",
    "# https://keras.io/preprocessing/sequence/\n",
    "# https://github.com/keras-team/keras/blob/master/keras/preprocessing/sequence.py#L248\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize text and create dictionary mapping tokens to integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_all.review)\n",
    "\n",
    "# create sequences of integers to represent reviews, and find longest sentence\n",
    "seqs = tokenizer.texts_to_sequences(df_all.review)\n",
    "max_len = max([len(seq) for seq in seqs])\n",
    "\n",
    "# pad sequences to feed to the embedding layer\n",
    "seqs = pad_sequences(seqs, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents  = 10662\n",
      "Size of vocabulary   = 19498\n",
      "Maximum sequence len = 51\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents  = {}'.format(tokenizer.document_count))\n",
    "print('Size of vocabulary   = {}'.format(len(tokenizer.word_index)))\n",
    "print('Maximum sequence len = {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "The model will have 2 branches that will be merged on axis = 1, then fed to a final 1 deep fully connect MLP layer\n",
    "\n",
    "Branch 1 will be for LSTM using embedding layer with vectorized representation of tokens <br>\n",
    "Branch 2 will be for CNN using embedding layer with vectorized representation of tokens <br>\n",
    "\n",
    "For branch 2 I will use \"ConvNet Architectures\" guidelines from Stanford \"CS231n: Convolutional Neural Networks for Visual Recognition\" class guidelines: http://cs231n.github.io/convolutional-networks/ <br>\n",
    "\"INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC <br>\n",
    "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)...\"\n",
    "\n",
    "In this case N=1, M=1 and K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 51, 64)       1247936     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 51)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 51, 128)      1572992     embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 51, 64)       1247936     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 51, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 51, 200)      132000      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 26, 128)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10200)        0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 3328)         0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           163216      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           53264       flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32)           0           dropout_9[0][0]                  \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           528         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            17          dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,417,889\n",
      "Trainable params: 4,417,889\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = max_len\n",
    "VOC_SIZE1 = len(tokenizer.word_index)\n",
    "VOC_SIZE2 = VOC_SIZE1\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras import regularizers\n",
    "\n",
    "def get_model(output_dim=64, filter_size=128, window_size=3, stride=1, pool_size=2, dense_dim=16):\n",
    "    \n",
    "    # Branch 1\n",
    "    input1 = Input(shape=(MAX_SEQ,), dtype='float64')\n",
    "    embed1 = Embedding(VOC_SIZE1+1, output_dim, input_length=MAX_SEQ, embeddings_initializer='random_uniform')(input1)\n",
    "    lstm = Bidirectional(LSTM(units=100, return_sequences=True))(embed1)\n",
    "    lstm = Flatten()(lstm)\n",
    "    D1 = Dense(dense_dim, activation='relu')(lstm)\n",
    "    D1 = Dropout(0.5)(D1)\n",
    "    \n",
    "    # Branch2\n",
    "    krn_size = output_dim * window_size\n",
    "    input2 = Input(shape=(MAX_SEQ,), dtype='int32')\n",
    "    embed2 = Embedding(VOC_SIZE2+1, output_dim, input_length=MAX_SEQ, embeddings_initializer='random_uniform')(input2)\n",
    "    C2 = Conv1D(filter_size, kernel_size=krn_size ,padding='same', strides=(stride), activation='relu')(embed2)\n",
    "    C2 = Dropout(0.5)(C2)\n",
    "    M2 = MaxPooling1D(pool_size=(pool_size), padding='same')(C2)\n",
    "    F2 = Flatten()(M2)\n",
    "    D2 = Dense(dense_dim, activation='relu')(F2)\n",
    "    D2 = Dropout(0.5)(D2)\n",
    "    \n",
    "    # Merge branches by concatenating along axis 1\n",
    "    L1 = concatenate([D1, D2], axis=1)\n",
    "    G1= Dense(dense_dim, activation='relu')(L1)\n",
    "    G1 = Dropout(0.5)(G1)\n",
    "\n",
    "    pred = Dense(1, kernel_regularizer=regularizers.l2(0.01),\\\n",
    "                 activity_regularizer=regularizers.l1(0.01),\\\n",
    "                 activation='sigmoid')(G1)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9594 samples, validate on 1068 samples\n",
      "Epoch 1/20\n",
      " - 89s - loss: 1.2232 - acc: 0.5006 - val_loss: 1.1278 - val_acc: 0.5028\n",
      "Epoch 2/20\n",
      " - 84s - loss: 1.0532 - acc: 0.6356 - val_loss: 1.0058 - val_acc: 0.5974\n",
      "Epoch 3/20\n",
      " - 82s - loss: 0.8934 - acc: 0.7575 - val_loss: 0.9998 - val_acc: 0.7022\n",
      "Epoch 4/20\n",
      " - 85s - loss: 0.7856 - acc: 0.8499 - val_loss: 1.1399 - val_acc: 0.7612\n",
      "\n",
      "\t>> Score for split 1: 0.7612359550561798\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 86s - loss: 1.2783 - acc: 0.4972 - val_loss: 1.1533 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 93s - loss: 1.1266 - acc: 0.5719 - val_loss: 1.0305 - val_acc: 0.6623\n",
      "Epoch 3/20\n",
      " - 92s - loss: 0.9601 - acc: 0.7208 - val_loss: 1.0314 - val_acc: 0.7111\n",
      "\n",
      "\t>> Score for split 2: 0.7110694183864915\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 86s - loss: 1.2195 - acc: 0.4991 - val_loss: 1.0849 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 85s - loss: 1.0407 - acc: 0.6110 - val_loss: 1.0058 - val_acc: 0.6323\n",
      "Epoch 3/20\n",
      " - 84s - loss: 0.8613 - acc: 0.7956 - val_loss: 1.0364 - val_acc: 0.7092\n",
      "\n",
      "\t>> Score for split 3: 0.7091932457786116\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 93s - loss: 1.2337 - acc: 0.4976 - val_loss: 1.1632 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 88s - loss: 1.0650 - acc: 0.5791 - val_loss: 1.0271 - val_acc: 0.6876\n",
      "Epoch 3/20\n",
      " - 84s - loss: 0.8896 - acc: 0.7789 - val_loss: 1.0595 - val_acc: 0.7467\n",
      "\n",
      "\t>> Score for split 4: 0.7467166979362101\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 95s - loss: 1.2426 - acc: 0.5011 - val_loss: 1.1560 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 82s - loss: 1.0975 - acc: 0.5576 - val_loss: 1.0348 - val_acc: 0.6923\n",
      "Epoch 3/20\n",
      " - 93s - loss: 0.9149 - acc: 0.7630 - val_loss: 1.0578 - val_acc: 0.7214\n",
      "\n",
      "\t>> Score for split 5: 0.7213883677298312\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 89s - loss: 1.2520 - acc: 0.4978 - val_loss: 1.1800 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 80s - loss: 1.1272 - acc: 0.5239 - val_loss: 1.0173 - val_acc: 0.5647\n",
      "Epoch 3/20\n",
      " - 81s - loss: 0.9367 - acc: 0.7097 - val_loss: 0.9919 - val_acc: 0.7298\n",
      "Epoch 4/20\n",
      " - 79s - loss: 0.8136 - acc: 0.8581 - val_loss: 1.0287 - val_acc: 0.7570\n",
      "\n",
      "\t>> Score for split 6: 0.7570356472795498\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 86s - loss: 1.1948 - acc: 0.5015 - val_loss: 1.0554 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 79s - loss: 1.0033 - acc: 0.5026 - val_loss: 1.0249 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      " - 81s - loss: 0.8589 - acc: 0.5598 - val_loss: 1.0775 - val_acc: 0.7345\n",
      "\n",
      "\t>> Score for split 7: 0.7345215759849906\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 83s - loss: 1.2292 - acc: 0.4970 - val_loss: 1.1492 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 82s - loss: 1.0562 - acc: 0.5869 - val_loss: 1.0427 - val_acc: 0.6238\n",
      "Epoch 3/20\n",
      " - 242s - loss: 0.8798 - acc: 0.7691 - val_loss: 1.0236 - val_acc: 0.7289\n",
      "Epoch 4/20\n",
      " - 87s - loss: 0.7814 - acc: 0.8514 - val_loss: 1.2364 - val_acc: 0.7411\n",
      "\n",
      "\t>> Score for split 8: 0.7410881801125704\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 85s - loss: 1.2392 - acc: 0.4973 - val_loss: 1.0892 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 80s - loss: 1.0671 - acc: 0.5871 - val_loss: 1.0138 - val_acc: 0.6707\n",
      "Epoch 3/20\n",
      " - 80s - loss: 0.8866 - acc: 0.7575 - val_loss: 1.0599 - val_acc: 0.6904\n",
      "\n",
      "\t>> Score for split 9: 0.6904315196998124\n",
      "\n",
      "Train on 9596 samples, validate on 1066 samples\n",
      "Epoch 1/20\n",
      " - 85s - loss: 1.2676 - acc: 0.5033 - val_loss: 1.1818 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      " - 79s - loss: 1.1370 - acc: 0.5936 - val_loss: 1.0365 - val_acc: 0.6248\n",
      "Epoch 3/20\n",
      " - 79s - loss: 0.9594 - acc: 0.7390 - val_loss: 1.0231 - val_acc: 0.6839\n",
      "Epoch 4/20\n",
      " - 84s - loss: 0.8293 - acc: 0.8266 - val_loss: 1.0585 - val_acc: 0.7083\n",
      "\n",
      "\t>> Score for split 10: 0.7082551594746717\n",
      "\n",
      "Average accuracy = 0.7280935767438919\n",
      "CPU times: user 5h 16min, sys: 13min 41s, total: 5h 29min 41s\n",
      "Wall time: 51min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# evaluate using 10-fold CV as in Yoon Kim article\n",
    "FOLDS = 10\n",
    "\n",
    "# concatenates input data so kfold splits all data as one\n",
    "X = np.hstack((seqs, seqs))\n",
    "y = np.array(df_all.rating.tolist())\n",
    "kfold = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "acc = list()\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X, y):\n",
    "    i += 1\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "    model = get_model()\n",
    "    # divide input back into seqs and tag_seqs\n",
    "    model.fit([X_train[:,:MAX_SEQ], X_train[:,MAX_SEQ:]], y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, \\\n",
    "              validation_data=([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]], y_valid),\\\n",
    "              callbacks=[EarlyStopping(patience=1, monitor='val_loss')])\n",
    "    y_hat = model.predict([X_valid[:,:MAX_SEQ], X_valid[:,MAX_SEQ:]])\n",
    "    y_pred = [round(pred) for pred in y_hat.reshape(-1)]\n",
    "    acc.append(accuracy_score(y_valid, y_pred))\n",
    "    print('\\n\\t>> Score for split {}: {}\\n'.format(i, acc[-1]))\n",
    "\n",
    "print('Average accuracy = {}'.format(np.mean(np.array(acc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
