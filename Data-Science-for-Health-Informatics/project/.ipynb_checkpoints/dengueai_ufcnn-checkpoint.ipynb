{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFCNN, San Juan, total cases is only feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import theano \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import Dense, Activation, Dropout, Convolution1D, Merge\n",
    "%matplotlib inline\n",
    "import myutil_ufcnn as myutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data.  Will use total_cases as single input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearweekofyear</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199018</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199019</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199020</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199021</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199022</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city  year  weekofyear  total_cases\n",
       "yearweekofyear                                    \n",
       "199018           sj  1990          18            4\n",
       "199019           sj  1990          19            5\n",
       "199020           sj  1990          20            4\n",
       "199021           sj  1990          21            3\n",
       "199022           sj  1990          22            6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input = myutil.get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "all_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and shape San Juan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936, 1, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sj = all_input[all_input['city']=='sj']['total_cases'].values\n",
    "input_sj = input_sj.reshape(input_sj.shape[0],1,1)\n",
    "input_sj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and shape Iquitos data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 1, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_iq = all_input[all_input['city']=='iq']['total_cases'].values\n",
    "input_iq = input_iq.reshape(input_iq.shape[0],1,1)\n",
    "input_iq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 1==2:\n",
    "    for city in best_result.keys():\n",
    "        print('Best result for {}'.format(city))\n",
    "        print('Best dataset: {}'.format(best_result[city][1]))\n",
    "        print('\\t\\tFinal loss train: {}'.format(best_result[city][2]))\n",
    "        print('\\t\\tFinal loss valid: {}'.format(best_result[city][3]))\n",
    "        print('\\t\\tVariance score: %.2f' % best_result[city][4])\n",
    "        plt.plot(best_result[city][0]['loss'], label='train')\n",
    "        plt.plot(best_result[city][0]['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_nodes_shapes(model):\n",
    "    for layer in model.layers:\n",
    "        for attrib in layer:\n",
    "            print(attrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_predictions(raw_predictions):\n",
    "    raw_predictions[ raw_predictions < 0] = 0\n",
    "    raw_predictions = np.around(raw_predictions).astype('int')\n",
    "    return raw_predictions\n",
    "    \n",
    "def run_ufcnn(X_train_, X_valid_, y_train_, y_valid_, model_parms):\n",
    "    \n",
    "    sequence_length=X_train_.shape[0]\n",
    "    features=1\n",
    "    nb_filter=int(sequence_length/4)     # Number of convolution kernels to use (dimensionality of the output)\n",
    "    filter_length=2                      # The extension (spatial or temporal) of each filter\n",
    "    output_dim=1\n",
    "    optimizer='adagrad'\n",
    "    loss='mse'\n",
    "    input_shape=(1,1)\n",
    "    \n",
    "    X_train_ = X_train_[:,-1:].reshape(X_train_.shape[0], 1, 1)\n",
    "    X_valid_ = X_valid_[:,-1:].reshape(X_valid_.shape[0], 1, 1)\n",
    "    \n",
    "    #print(X_train.shape)\n",
    "\n",
    "    # build graph\n",
    "    H1 = Sequential()\n",
    "    H1.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\n",
    "                         activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    H2 = Sequential()\n",
    "    H2.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    H2.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    \n",
    "    H3 = Sequential()\n",
    "    H3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    H3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    H3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    \n",
    "    G3 = Sequential()\n",
    "    G3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    G3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    G3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))\n",
    "    G3.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features)))  \n",
    "    \n",
    "    merged = Merge([H2, G3], mode='sum')\n",
    "    G2 = Sequential()\n",
    "    G2.add(merged)\n",
    "    G2.add(Activation('relu'))\n",
    "    \n",
    "    merged = Merge([H1, G2], mode='sum')\n",
    "    G1 = Sequential()\n",
    "    G1.add(merged)\n",
    "    G1.add(Activation('relu'))\n",
    "    G1.add(Convolution1D(nb_filter=output_dim, filter_length=filter_length, border_mode='same',\\\n",
    "                         activation='relu', input_shape=(sequence_length, features))) \n",
    "    \n",
    "    history = regr.fit(X_train_, y_train_, epochs=20, batch_size=2,\\\n",
    "                       validation_data=(X_valid_, y_valid_), verbose=0, shuffle=False)\n",
    "    \n",
    "    y_hat = fix_predictions(G1.predict(X_train_))\n",
    "    y_pred = fix_predictions(G1.predict(X_valid_))\n",
    "\n",
    "    loss_train = history.history['loss'][-1]\n",
    "    loss_valid = history.history['val_loss'][-1]\n",
    "    r2 = r2_score(y_valid_, y_pred)\n",
    "    score = r2/loss_valid\n",
    "    \n",
    "    return G1, score, r2, loss_train, loss_valid, history.history\n",
    "    #       0      1     2      3          4             5\n",
    "\n",
    "    \n",
    "def score_dataset(X_train, y_train):\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.33, random_state=42)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_results = (0, 0, 0, 0, 0, 0)\n",
    "    \n",
    "    params = ['dummy'] \n",
    "    for parms in params:        \n",
    "        model, this_score, r2, loss_train, loss_valid, _ = run_ufcnn(X_train, X_valid, y_train, y_valid, parms)\n",
    "        # only check scores if loss_train < loss_valid\n",
    "        if (loss_train < loss_valid) and (this_score > best_score):\n",
    "            best_results = params, model, this_score, r2, loss_train, loss_valid\n",
    "            #print('Database: {} / Train Loss: {} / Valid Loss {} / R2: {}'.format(id, loss_train, loss_valid, r2))  \n",
    "    return best_results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sj_8_raw\n",
      "(924, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only layers of same output shape can be merged using sum mode. Layer shapes: [(None, 1, 154), (None, 619, 154)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f8c8a3601d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# get model and best score data for this dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# save best score so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthis_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-353f102c0b03>\u001b[0m in \u001b[0;36mscore_dataset\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'dummy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ufcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m# only check scores if loss_train < loss_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mloss_valid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthis_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-353f102c0b03>\u001b[0m in \u001b[0;36mrun_ufcnn\u001b[0;34m(X_train_, X_valid_, y_train_, y_valid_, model_parms)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mG2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mH1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mG1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mG1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dsdht/lib/python3.6/site-packages/keras/legacy/layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, node_indices, tensor_indices, name)\u001b[0m\n\u001b[1;32m    108\u001b[0m             self._arguments_validation(layers, mode,\n\u001b[1;32m    109\u001b[0m                                        \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                                        node_indices, tensor_indices)\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/dsdht/lib/python3.6/site-packages/keras/legacy/layers.py\u001b[0m in \u001b[0;36m_arguments_validation\u001b[0;34m(self, layers, mode, concat_axis, dot_axes, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 raise ValueError('Only layers of same output shape can '\n\u001b[1;32m    153\u001b[0m                                  \u001b[0;34m'be merged using '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' mode. '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                                  'Layer shapes: %s' % input_shapes)\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'cos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dot'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only layers of same output shape can be merged using sum mode. Layer shapes: [(None, 1, 154), (None, 619, 154)]"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_results = ()\n",
    "\n",
    "for dataset_name in dataset.keys():\n",
    "    print(dataset_name)\n",
    "    # get partitions for this version of the dataset\n",
    "    X_train, y_train = dataset[dataset_name]['X_train'], dataset[dataset_name]['y_train']\n",
    "    # get model and best score data for this dataset\n",
    "    params, model, this_score, r2, loss_train, loss_valid = score_dataset(X_train, y_train)\n",
    "    # save best score so far\n",
    "    if this_score > best_score:\n",
    "        best_score = dataset_name, params, model, this_score, r2, loss_train, loss_valid\n",
    "        print('Database: {} / Train Loss: {} / Valid Loss {} / R2: {}'.format(dataset_name, loss_train, loss_valid, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
