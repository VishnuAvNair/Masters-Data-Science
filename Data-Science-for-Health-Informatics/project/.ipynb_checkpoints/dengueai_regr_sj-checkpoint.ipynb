{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import myutil_regr as myutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# iq training data\n",
    "#\n",
    "dfx_train_iq = pd.read_csv('data/dengue_features_train_no_outliers_iq.csv')\n",
    "dfx_train_iq.set_index('yearweekofyear', inplace=True)\n",
    "dfy_train_iq = pd.read_csv('data/dengue_labels_train_iq.csv')\n",
    "dfy_train_iq.set_index('yearweekofyear', inplace=True)\n",
    "#dftrain_iq = pd.merge(dfx_train_iq, dfy_train_iq)\n",
    "#dftrain_iq.set_index('yearweekofyear', inplace=True)\n",
    "#\n",
    "# sj training data\n",
    "# \n",
    "dfx_train_sj = pd.read_csv('data/dengue_features_train_no_outliers_sj.csv')\n",
    "dfx_train_sj.set_index('yearweekofyear', inplace=True)\n",
    "dfy_train_sj = pd.read_csv('data/dengue_labels_train_sj.csv')\n",
    "dfy_train_sj.set_index('yearweekofyear', inplace=True)\n",
    "#dftrain_sj = pd.merge(dfx_train_sj, dfy_train_sj)\n",
    "#dftrain_sj.set_index('yearweekofyear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regr_run(X, y, poly_degree=1, exploring=False):\n",
    "        \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.33, random_state=42)\n",
    "        \n",
    "    if poly_degree > 1:\n",
    "        poly = PolynomialFeatures(poly_degree, interaction_only=True)\n",
    "        X_train = poly.fit_transform(X_train)\n",
    "        X_valid = poly.fit_transform(X_valid)\n",
    "        \n",
    "    if exploring: print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    #regr = linear_model.Ridge(alpha = .5)\n",
    "    #regr = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "    #regr = linear_model.Lasso(alpha = .1)\n",
    "    #regr = linear_model.LassoLars(alpha = .1)\n",
    "    #regr = linear_model.BayesianRidge()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_valid)\n",
    "    #y_pred[ y_pred < 0] = 0\n",
    "    y_pred = np.around(y_pred).astype('int')\n",
    "    \n",
    "    #print(np.hstack((y_valid, y_pred.reshape(y_pred.shape[0],1))))\n",
    "    \n",
    "    # The coefficients\n",
    "    #print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean absolute error: %.2f\" % mean_absolute_error(y_valid, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_valid, y_pred))\n",
    "    \n",
    "    return regr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First make predictions without feature total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 36) (171, 36) (347, 1) (171, 1)\n",
      "Mean absolute error: 7.25\n",
      "Variance score: 0.02\n"
     ]
    }
   ],
   "source": [
    "# total cases is rightmost column\n",
    "dfx_iq = dfx_train_iq.iloc[:,:-1].copy()\n",
    "\n",
    "periods_iq = 2    # best 1\n",
    "degree_iq = 1     # best 1\n",
    "# scaler 1 for entire feature set\n",
    "# scaler 2 for total_cases only\n",
    "X_iq, scaler_iq, scaler_tc_iq = myutil.preprocess(dfx_iq, periods_iq)\n",
    "y_iq = dfy_train_iq.values[periods_iq:,:]\n",
    "regr_iq= regr_run(X_iq, y_iq, degree_iq, exploring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(626, 18) (309, 18) (626, 1) (309, 1)\n",
      "Mean absolute error: 27.98\n",
      "Variance score: 0.05\n"
     ]
    }
   ],
   "source": [
    "# total cases is rightmost column\n",
    "dfx_sj = dfx_train_sj.iloc[:,:-1].copy()\n",
    "\n",
    "periods_sj = 1    # best 2\n",
    "degree_sj = 1     # best 1\n",
    "# scaler 1 for entire feature set\n",
    "# scaler 2 for total_cases only\n",
    "X_sj, scaler_sj, scaler_tc_sj = myutil.preprocess(dfx_sj, periods_sj)\n",
    "y_sj = dfy_train_sj.values[periods_sj:,:]\n",
    "regr_sj = regr_run(X_sj, y_sj, degree_sj, exploring=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then make predictions with feature total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 19) (172, 19) (347, 1) (172, 1)\n",
      "Mean absolute error: 4.16\n",
      "Variance score: 0.49\n"
     ]
    }
   ],
   "source": [
    "periods_iq = 1    # best 1\n",
    "degree_iq = 1     # best 1\n",
    "# scaler 1 for entire feature set\n",
    "# scaler 2 for total_cases only\n",
    "X_iq, scaler_iq, scaler_tc_iq = myutil.preprocess(dfx_train_iq.copy(), periods_iq)\n",
    "y_iq = dfy_train_iq.values[periods_iq:,:]\n",
    "regr_iq= regr_run(X_iq, y_iq, degree_iq, exploring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(626, 19) (309, 19) (626, 1) (309, 1)\n",
      "Mean absolute error: 13.48\n",
      "Variance score: 0.66\n"
     ]
    }
   ],
   "source": [
    "periods_sj = 1    # best 2\n",
    "degree_sj = 1     # best 1\n",
    "# scaler 1 for entire feature set\n",
    "# scaler 2 for total_cases only\n",
    "X_sj, scaler_sj, scaler_tc_sj = myutil.preprocess(dfx_train_sj.copy(), periods_sj)\n",
    "y_sj = dfy_train_sj.values[periods_sj:,:]\n",
    "regr_sj = regr_run(X_sj, y_sj, degree_sj, exploring=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test dataset and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_test_iq = pd.read_csv('data/dengue_features_test_no_outliers_iq.csv')\n",
    "dfx_test_iq.set_index('yearweekofyear', inplace=True)\n",
    "dfx_test_sj = pd.read_csv('data/dengue_features_test_no_outliers_sj.csv')\n",
    "dfx_test_sj.set_index('yearweekofyear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict San Juan first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0231936]]\n"
     ]
    }
   ],
   "source": [
    "# first prediction will require periods_sj data from training dataset along with training labels\n",
    "feature_count_sj = len(dfx_train_sj.columns)\n",
    "df_last_train_sj = dfx_train_sj.iloc[-periods_sj:,:].values.reshape(1,periods_sj*feature_count_sj)\n",
    "X_last_train_sj = scaler_sj.transform(df_last_train_sj)\n",
    "# predict, scale, set to zero if needed\n",
    "y_pred = max([[0]],scaler_tc_sj.transform(y_sj[-1:,:]))\n",
    "print(scaler_tc_sj.transform(regr_sj.predict(X_last_train_sj).reshape(1,1)))\n",
    "\n",
    "# now, for every row in test dataset, append prior prediction to get a new one\n",
    "predictions_sj = list()\n",
    "sick_sj = y_pred\n",
    "nptest_sj = dfx_test_sj.values\n",
    "for i in range(0, nptest_sj.shape[0]):\n",
    "    #print(sick_sj)\n",
    "    X_sj = scaler_sj.transform(np.hstack((nptest_sj[i:i+1], sick_sj)))\n",
    "    sick_sj = max([[0]],scaler_tc_sj.transform(regr_sj.predict(X_sj).reshape(1,1)))\n",
    "    predictions_sj.append(int(round(sick_sj[0][0])))\n",
    "np_predictions_sj = np.array(predictions_sj).reshape(len(predictions_sj),1)\n",
    "#np_predictions_sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Iquito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first prediction will require periods_sj data from training dataset along with training labels\n",
    "feature_count_iq = len(dfx_train_iq.columns)\n",
    "df_last_train_iq = dfx_train_iq.iloc[-periods_iq:,:].values.reshape(1,periods_iq*feature_count_iq)\n",
    "X_last_train_iq = scaler_iq.transform(df_last_train_iq)\n",
    "# predict, scale, set to zero if needed\n",
    "y_pred = max([[0]],scaler_tc_iq.transform(regr_iq.predict(X_last_train_iq).reshape(1,1)))\n",
    "\n",
    "# now, for every row in test dataset, append prior prediction to get a new one\n",
    "predictions_iq = list()\n",
    "sick_iq = y_pred\n",
    "nptest_iq = dfx_test_iq.values\n",
    "for i in range(0, nptest_iq.shape[0]):\n",
    "    #print(sick_iq)\n",
    "    sick_iq = np.array(y_pred).reshape(1,1)\n",
    "    X_iq = scaler_iq.transform(np.hstack((nptest_iq[i:i+1], sick_iq)))\n",
    "    sick_iq = max([[0]],scaler_tc_iq.transform(regr_iq.predict(X_iq).reshape(1,1)))\n",
    "    predictions_iq.append(int(round(sick_iq[0][0])))\n",
    "np_predictions_iq = np.array(predictions_iq).reshape(len(predictions_iq),1)\n",
    "np_predictions_iq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsubm = pd.read_csv('data/submission_format.csv')\n",
    "npsubm_sj = np.concatenate((dfsubm[dfsubm['city']=='sj'][['city','year','weekofyear']].values, \\\n",
    "                            np_predictions_sj), axis=1)\n",
    "npsubm_iq = np.concatenate((dfsubm[dfsubm['city']=='iq'][['city','year','weekofyear']].values, \\\n",
    "                            np_predictions_iq), axis=1)\n",
    "dfresults = pd.DataFrame(np.concatenate((npsubm_sj, npsubm_iq), axis=0), columns=dfsubm.columns)\n",
    "dfresults.to_csv(\"data/submission_20171119_regr_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_predict_and_save(dftrain_iq, regr_iq, periods_iq, dftrain_sj, regr_sj, periods_sj, dftest_iq, dftest_sj,\\\n",
    "                      \"data/submission_20171116_regr_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
