{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three model types, several dataset versions, total_cases IS THE ONLY FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import theano \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, GRU, Embedding, Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "%matplotlib inline\n",
    "import myutil_all as myutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and do preliminary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx_train = myutil.get_indexed_dataset('data/dengue_features_train.csv')\n",
    "dfy_train = myutil.get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "dfx_test = myutil.get_indexed_dataset('data/dengue_features_test.csv')\n",
    "# combine training features with training labels for data exploration later on\n",
    "dftrain = myutil.set_index(pd.merge(dfx_train, dfy_train))\n",
    "# Will stack the train and test datasets to treat all NaN values together\n",
    "# Need to add bogus total_cases column to test dataset so the files can be easily concatenated\n",
    "# update total_cases = -1 to easily identify the records for later split data to original partitions\n",
    "dfx_test['total_cases'] = -1\n",
    "dfall = myutil.set_index(pd.concat((dftrain, dfx_test), axis=0))\n",
    "dfall.sort_index(axis=0, inplace=True)\n",
    "# drop unecessary columns and save column names\n",
    "delcols = ['year','week_start_date','reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg']\n",
    "dfall.drop(delcols, axis=1, inplace=True)\n",
    "cols = dfall.columns\n",
    "# remove NaNs\n",
    "dfall = myutil.set_nan_to_week_mean(dfall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode city and weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset and hot encode weekof year\n",
    "dfall_copy = dfall.copy()\n",
    "dfall_iq = dfall_copy[dfall_copy['city'] == 'iq']\n",
    "dfall_sj = dfall_copy[dfall_copy['city'] == 'sj']\n",
    "enc = OneHotEncoder(categorical_features=np.array([0]))\n",
    "dset = dict()\n",
    "dset['iq'] = enc.fit_transform(dfall_iq.iloc[:,1:].values).toarray()\n",
    "dset['sj'] = enc.fit_transform(dfall_sj.iloc[:,1:].values).toarray()\n",
    "dset['iq'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create several versions of the datasets (ADD total cases as ONLY feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each city, create version of datasets with 1 to 16 shifts (3 months)\n",
    "for city in dset.keys():\n",
    "    datashift = dict()\n",
    "    for shift_no in range(1,17):\n",
    "        # total cases is THE ONLY FEATURE\n",
    "        datashift[shift_no] = myutil.shift(np.hstack((dset[city][:,-1:],dset[city][:,-1:])), shift_no)\n",
    "    dset[city] = datashift\n",
    "\n",
    "# for each city, each shift, create version of the dataset different scaling\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        vers = dict()\n",
    "        # save original as 'raw'\n",
    "        vers['raw'] = np.hstack((dset[city][shift_no][:,:-1], dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        vers['minmax1'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        # isolate features, so scaling will only affect features\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        vers['minmax2'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        dset[city][shift_no] = vers\n",
    "        \n",
    "# for each city, each shift, each version, create X_train, y_train, and X_test partitions\n",
    "dataset = dict()\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        for vers in dset[city][shift_no].keys():\n",
    "            npdata = dset[city][shift_no][vers]\n",
    "            partition = dict()\n",
    "            partition['X_train'] = npdata[npdata[:,-1]>0][:,:-1]\n",
    "            partition['y_train'] = npdata[npdata[:,-1]>0][:,-1:]\n",
    "            partition['X_test']  = npdata[npdata[:,-1]<0][:,:-1]\n",
    "            id = city + '_' + str(shift_no) + '_' + vers\n",
    "            dataset[id] = partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for key in dataset.keys():\n",
    "#    print(key[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result for iq\n",
      "\tBest dataset: iq_15_raw\n",
      "\tBest model  : BayesianRidge\n",
      "\t\tMAE train : 4.14\n",
      "\t\tMAE valid : 4.70\n",
      "\t\tVariance  : 0.50\n",
      "Best result for sj\n",
      "\tBest dataset: sj_11_minmax1\n",
      "\tBest model  : Lasso\n",
      "\t\tMAE train : 8.67\n",
      "\t\tMAE valid : 7.68\n",
      "\t\tVariance  : 0.90\n"
     ]
    }
   ],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "    \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "    # Create linear regression object\n",
    "    for i in range(0, 6):\n",
    "        if i==0: \n",
    "            key = 'LinearRegression'\n",
    "            regr = linear_model.LinearRegression()\n",
    "        if i==1:\n",
    "            key = 'Ridge'\n",
    "            regr = linear_model.Ridge(alpha = .5)\n",
    "        if i==2:\n",
    "            key = 'RidgeCV'  \n",
    "            regr = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "        if i==3:\n",
    "            key = 'Lasso'\n",
    "            regr = linear_model.Lasso(alpha = .1)\n",
    "        if i==4:\n",
    "            key = 'LassoLars'\n",
    "            regr = linear_model.LassoLars(alpha = .1)\n",
    "        if i==5:\n",
    "            key = 'BayesianRidge'  \n",
    "            regr = linear_model.BayesianRidge()\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X_train_, y_train_.ravel())\n",
    "\n",
    "        y_hat = regr.predict(X_train_)\n",
    "        y_hat[ y_hat < 0] = 0\n",
    "        y_hat = np.around(y_hat).astype('int')\n",
    "        y_pred = regr.predict(X_valid_)\n",
    "        y_pred[ y_pred < 0] = 0\n",
    "        y_pred = np.around(y_pred).astype('int')\n",
    "\n",
    "        # scores are training mae, validation mae, variance\n",
    "        mae_train = mean_absolute_error(y_train_, y_hat)\n",
    "        mae_valid = mean_absolute_error(y_valid_, y_pred)\n",
    "        r2        = r2_score(y_valid_, y_pred)\n",
    "\n",
    "        if abs(r2) < 1:\n",
    "            this_score = abs(r2/mae_valid)\n",
    "        else: \n",
    "            this_score = 0\n",
    "            \n",
    "        if id[:2] == 'iq':\n",
    "            if this_score > best_score['iq']:\n",
    "                best_score['iq'] = this_score\n",
    "                best_result['iq'] = (regr, id, key, mae_train, mae_valid, r2)\n",
    "        else:\n",
    "            if this_score > best_score['sj']:\n",
    "                best_score['sj'] = this_score\n",
    "                best_result['sj'] = (regr, id, key, mae_train, mae_valid, r2)\n",
    "\n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('\\tBest dataset: {}'.format(best_result[city][1]))\n",
    "    print('\\tBest model  : {}'.format(best_result[city][2]))\n",
    "    print('\\t\\tMAE train : %.2f' % best_result[city][3])\n",
    "    print('\\t\\tMAE valid : %.2f' % best_result[city][4])\n",
    "    print('\\t\\tVariance  : %.2f' % best_result[city][5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisition tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "        \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "        \n",
    "    #RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "    #                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, \n",
    "    #                      max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "    #                      bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "    #                      warm_start=False)\n",
    "    \n",
    "    #regr = RandomForestRegressor(n_estimators=30, max_depth=max_depth, criterion='mse', random_state=0,\\\n",
    "    #                            min_samples_leaf=1)\n",
    "    \n",
    "    #AdaBoostRegressor(base_estimator=None, n_estimators=150, learning_rate=1.0, loss=’linear’, random_state=None)\n",
    "    \n",
    "    regr = AdaBoostRegressor(RandomForestRegressor(max_depth=6), n_estimators=200, learning_rate=0.01, \\\n",
    "                             loss='exponential', random_state=42)\n",
    "    \n",
    "    regr.fit(X_train_, y_train_.ravel())\n",
    "\n",
    "    y_hat = regr.predict(X_train_)\n",
    "    y_hat[ y_hat < 0] = 0\n",
    "    y_hat = np.around(y_hat).astype('int')\n",
    "    y_pred = regr.predict(X_valid_)\n",
    "    y_pred[ y_pred < 0] = 0\n",
    "    y_pred = np.around(y_pred).astype('int')\n",
    "\n",
    "    # scores are training mae, validation mae, variance\n",
    "    mae_train = mean_absolute_error(y_train_, y_hat)\n",
    "    mae_valid = mean_absolute_error(y_valid_, y_pred)\n",
    "    r2        = r2_score(y_valid_, y_pred)\n",
    "\n",
    "    this_score = abs(r2/mae_valid)\n",
    "    if id[:2] == 'iq':\n",
    "        if this_score > best_score['iq']:\n",
    "            best_score['iq'] = this_score\n",
    "            best_result['iq'] = (regr, id, mae_train, mae_valid, r2)\n",
    "    else:\n",
    "        if this_score > best_score['sj']:\n",
    "            best_score['sj'] = this_score\n",
    "            best_result['sj'] = (regr, id, mae_train, mae_valid, r2)\n",
    "    \n",
    "    #plt.scatter(y_valid, y_pred)\n",
    "    \n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('\\tBest dataset: {}'.format(best_result[city][1]))\n",
    "    #print('\\tBest model  : {}'.format(best_result[city][2]))\n",
    "    print('\\t\\tMAE train : %.2f' % best_result[city][2])\n",
    "    print('\\t\\tMAE valid : %.2f' % best_result[city][3])\n",
    "    print('\\t\\tVariance  : %.2f' % best_result[city][4])\n",
    "    print('Feature Importances:')\n",
    "    print(best_result[city][0].feature_importances_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "        \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "    \n",
    "    X_train_ = X_train_.reshape(X_train_.shape[0], int(id[3:4]), int(X_train_.shape[1]/int(id[3:4])))\n",
    "    X_valid_ = X_valid_.reshape(X_valid_.shape[0], int(id[3:4]), int(X_valid_.shape[1]/int(id[3:4])))\n",
    "    \n",
    "    # build graph\n",
    "    regr = Sequential()\n",
    "    \n",
    "    #keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "    #              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "    #              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "    #              unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #              bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "    #              recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
    "    #              recurrent_dropout=0.0, implementation=1, return_sequences=False, \n",
    "    #              return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "\n",
    "    regr.add(GRU(X_train_.shape[2], input_shape=(X_train_.shape[1], X_train_.shape[2]),\\\n",
    "                  implementation=2, return_sequences=True))\n",
    "    regr.add(GRU(5))\n",
    "    regr.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activation='linear'))\n",
    "    regr.compile(loss='mae', optimizer='rmsprop')  # 'adam'\n",
    "    \n",
    "    # fit net\n",
    "    history = regr.fit(X_train_, y_train_, epochs=20, batch_size=4,\\\n",
    "                       validation_data=(X_valid_, y_valid_), verbose=0, shuffle=False)\n",
    "    \n",
    "    y_hat = regr.predict(X_train_)\n",
    "    y_hat[ y_hat < 0] = 0\n",
    "    y_hat = np.around(y_hat).astype('int')\n",
    "    y_pred = regr.predict(X_valid_)\n",
    "    y_pred[ y_pred < 0] = 0\n",
    "    y_pred = np.around(y_pred).astype('int')\n",
    "    \n",
    "    loss_train = history.history['loss'][-1]\n",
    "    loss_valid = history.history['val_loss'][-1]\n",
    "    r2 = r2_score(y_valid_, y_pred)\n",
    "    \n",
    "    print('Database: {} / Train Loss: {} / Valid Loss {} / R2: {}'.format(id, loss_train, loss_valid, r2))\n",
    "\n",
    "    if loss_valid > loss_train:\n",
    "        this_score = abs(r2/loss_valid)\n",
    "        #this_score = loss_valid\n",
    "    else:\n",
    "        this_score = 0\n",
    "    \n",
    "    if id[:2] == 'iq':\n",
    "        if this_score > best_score['iq']:\n",
    "            best_score['iq'] = this_score\n",
    "            best_result['iq'] = (history.history, id, loss_train, loss_valid, r2)\n",
    "    else:\n",
    "        if this_score > best_score['sj']:\n",
    "            best_score['sj'] = this_score\n",
    "            best_result['sj'] = (history.history, id, loss_train, loss_valid, r2)\n",
    "        \n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('Best dataset: {}'.format(best_result[city][1]))\n",
    "    print('\\t\\tFinal loss train: {}'.format(best_result[city][2]))\n",
    "    print('\\t\\tFinal loss valid: {}'.format(best_result[city][3]))\n",
    "    print('\\t\\tVariance score: %.2f' % best_result[city][4])\n",
    "    plt.plot(best_result[city][0]['loss'], label='train')\n",
    "    plt.plot(best_result[city][0]['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
