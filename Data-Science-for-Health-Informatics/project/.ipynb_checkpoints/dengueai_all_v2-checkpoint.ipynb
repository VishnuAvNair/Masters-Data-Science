{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import theano \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras import regularizers\n",
    "%matplotlib inline\n",
    "import myutil_all as myutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and do preliminary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx_train = myutil.get_indexed_dataset('data/dengue_features_train.csv')\n",
    "dfy_train = myutil.get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "dfx_test = myutil.get_indexed_dataset('data/dengue_features_test.csv')\n",
    "# combine training features with training labels for data exploration later on\n",
    "dftrain = myutil.set_index(pd.merge(dfx_train, dfy_train))\n",
    "# Will stack the train and test datasets to treat all NaN values together\n",
    "# Need to add bogus total_cases column to test dataset so the files can be easily concatenated\n",
    "# update total_cases = -1 to easily identify the records for later split data to original partitions\n",
    "dfx_test['total_cases'] = -1\n",
    "dfall = myutil.set_index(pd.concat((dftrain, dfx_test), axis=0))\n",
    "dfall.sort_index(axis=0, inplace=True)\n",
    "# drop unecessary columns and save column names\n",
    "delcols = ['year','week_start_date','reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg']\n",
    "dfall.drop(delcols, axis=1, inplace=True)\n",
    "cols = dfall.columns\n",
    "# remove NaNs\n",
    "dfall = myutil.set_nan_to_week_mean(dfall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode city and weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset and hot encode weekof year\n",
    "dfall_copy = dfall.copy()\n",
    "dfall_iq = dfall_copy[dfall_copy['city'] == 'iq']\n",
    "dfall_sj = dfall_copy[dfall_copy['city'] == 'sj']\n",
    "enc = OneHotEncoder(categorical_features=np.array([0]))\n",
    "dset = dict()\n",
    "dset['iq'] = enc.fit_transform(dfall_iq.iloc[:,1:].values).toarray()\n",
    "dset['sj'] = enc.fit_transform(dfall_sj.iloc[:,1:].values).toarray()\n",
    "dset['iq'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create several versions of the datasets (DOES NOT add label as feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each city, create version of datasets with 1, 2, 3, and 4 shifts\n",
    "for city in dset.keys():\n",
    "    datashift = dict()\n",
    "    for shift_no in range(1,5):\n",
    "        datashift[shift_no] = myutil.shift(dset[city], shift_no)\n",
    "        #print(city, shift_no)\n",
    "        #print(datashift[shift_no].shape)\n",
    "    dset[city] = datashift\n",
    "\n",
    "# for each city, each shift, create version of the dataset different scaling\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        vers = dict()\n",
    "        # save original as 'raw'\n",
    "        vers['raw'] = np.hstack((dset[city][shift_no][:,:-1], dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        vers['minmax1'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        # isolate features, so scaling will only affect features\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        vers['minmax2'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        dset[city][shift_no] = vers\n",
    "        \n",
    "# for each city, each shift, each version, create X_train, y_train, and X_test partitions\n",
    "dataset = dict()\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        for vers in dset[city][shift_no].keys():\n",
    "            npdata = dset[city][shift_no][vers]\n",
    "            partition = dict()\n",
    "            partition['X_train'] = npdata[npdata[:,-1]>0][:,:-1]\n",
    "            partition['y_train'] = npdata[npdata[:,-1]>0][:,-1:]\n",
    "            partition['X_test']  = npdata[npdata[:,-1]<0][:,:-1]\n",
    "            id = city + '_' + str(shift_no) + '_' + vers\n",
    "            dataset[id] = partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for key in dataset.keys():\n",
    "#    print(key[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result for iq\n",
      "\tBest dataset: iq_3_minmax1\n",
      "\tBest model  : LinearRegression\n",
      "\t\tMAE train : 5.14\n",
      "\t\tMAE valid : 9.74\n",
      "\t\tVariance  : -0.13\n",
      "Best result for sj\n",
      "\tBest dataset: sj_3_minmax1\n",
      "\tBest model  : BayesianRidge\n",
      "\t\tMAE train : 24.40\n",
      "\t\tMAE valid : 24.23\n",
      "\t\tVariance  : 0.10\n"
     ]
    }
   ],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "    \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "    # Create linear regression object\n",
    "    for i in range(0, 6):\n",
    "        if i==0: \n",
    "            key = 'LinearRegression'\n",
    "            regr = linear_model.LinearRegression()\n",
    "        if i==1:\n",
    "            key = 'Ridge'\n",
    "            regr = linear_model.Ridge(alpha = .5)\n",
    "        if i==2:\n",
    "            key = 'RidgeCV'  \n",
    "            regr = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "        if i==3:\n",
    "            key = 'Lasso'\n",
    "            regr = linear_model.Lasso(alpha = .1)\n",
    "        if i==4:\n",
    "            key = 'LassoLars'\n",
    "            regr = linear_model.LassoLars(alpha = .1)\n",
    "        if i==5:\n",
    "            key = 'BayesianRidge'  \n",
    "            regr = linear_model.BayesianRidge()\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        regr.fit(X_train_, y_train_.ravel())\n",
    "\n",
    "        y_hat = regr.predict(X_train_)\n",
    "        y_hat[ y_hat < 0] = 0\n",
    "        y_hat = np.around(y_hat).astype('int')\n",
    "        y_pred = regr.predict(X_valid_)\n",
    "        y_pred[ y_pred < 0] = 0\n",
    "        y_pred = np.around(y_pred).astype('int')\n",
    "\n",
    "        # scores are training mae, validation mae, variance\n",
    "        mae_train = mean_absolute_error(y_train_, y_hat)\n",
    "        mae_valid = mean_absolute_error(y_valid_, y_pred)\n",
    "        r2        = r2_score(y_valid_, y_pred)\n",
    "\n",
    "        if abs(r2) < 1:\n",
    "            this_score = abs(r2/mae_valid)\n",
    "        else: \n",
    "            this_score = 0\n",
    "            \n",
    "        if id[:2] == 'iq':\n",
    "            if this_score > best_score['iq']:\n",
    "                best_score['iq'] = this_score\n",
    "                best_result['iq'] = (regr, id, key, mae_train, mae_valid, r2)\n",
    "        else:\n",
    "            if this_score > best_score['sj']:\n",
    "                best_score['sj'] = this_score\n",
    "                best_result['sj'] = (regr, id, key, mae_train, mae_valid, r2)\n",
    "\n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('\\tBest dataset: {}'.format(best_result[city][1]))\n",
    "    print('\\tBest model  : {}'.format(best_result[city][2]))\n",
    "    print('\\t\\tMAE train : %.2f' % best_result[city][3])\n",
    "    print('\\t\\tMAE valid : %.2f' % best_result[city][4])\n",
    "    print('\\t\\tVariance  : %.2f' % best_result[city][5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisition tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iq_1_raw\n",
      "iq_1_minmax1\n",
      "iq_1_minmax2\n",
      "iq_2_raw\n",
      "iq_2_minmax1\n",
      "iq_2_minmax2\n",
      "iq_3_raw\n",
      "iq_3_minmax1\n",
      "iq_3_minmax2\n",
      "iq_4_raw\n",
      "iq_4_minmax1\n",
      "iq_4_minmax2\n",
      "sj_1_raw\n",
      "sj_1_minmax1\n",
      "sj_1_minmax2\n",
      "sj_2_raw\n",
      "sj_2_minmax1\n",
      "sj_2_minmax2\n",
      "sj_3_raw\n",
      "sj_3_minmax1\n",
      "sj_3_minmax2\n",
      "sj_4_raw\n",
      "sj_4_minmax1\n",
      "sj_4_minmax2\n",
      "Best result for iq\n",
      "\tBest dataset: iq_1_minmax1\n",
      "\t\tMAE train : 4.14\n",
      "\t\tMAE valid : 7.52\n",
      "\t\tVariance  : 0.02\n",
      "Feature Importances:\n",
      "[  4.25099092e-02   1.36300479e-03   1.10688105e-02   4.10998387e-02\n",
      "   2.15264339e-02   2.52720641e-03   5.58359283e-03   1.09119431e-02\n",
      "   7.68978698e-05   9.39366276e-05   4.86018947e-05   3.73962641e-04\n",
      "   2.59877854e-04   2.76939528e-05   4.14960595e-05   6.49340824e-05\n",
      "   6.39243223e-05   7.01397967e-04   6.05761452e-05   8.22382234e-06\n",
      "   1.06218354e-04   4.52078296e-05   4.91892198e-05   8.84066046e-05\n",
      "   4.45668328e-06   5.14306050e-06   6.79469346e-05   1.04623951e-04\n",
      "   1.52382309e-05   1.63042440e-06   7.00206872e-06   1.48864932e-05\n",
      "   3.33729281e-05   2.22499728e-06   7.23848550e-04   5.74271098e-04\n",
      "   1.73722413e-03   3.24866435e-03   1.53266859e-04   1.21857347e-03\n",
      "   4.01059890e-04   1.62250856e-02   4.06562061e-02   2.21613842e-02\n",
      "   1.56936498e-02   1.02151831e-04   2.03019105e-05   1.24865278e-02\n",
      "   2.34827347e-03   1.40315130e-02   8.53199037e-05   3.28965616e-03\n",
      "   1.31795256e-03   4.97570316e-02   2.50847238e-02   2.45518033e-02\n",
      "   2.72395653e-02   2.28284567e-02   2.80273149e-02   2.66961638e-02\n",
      "   4.32073748e-02   4.26292379e-02   4.91159403e-02   5.79905388e-02\n",
      "   3.73365658e-02   5.60690898e-02   3.48447346e-02   3.05646009e-02\n",
      "   8.73673608e-02   3.65729508e-02   4.46838051e-02]\n",
      "Best result for sj\n",
      "\tBest dataset: sj_4_minmax2\n",
      "\t\tMAE train : 17.48\n",
      "\t\tMAE valid : 25.70\n",
      "\t\tVariance  : 0.12\n",
      "Feature Importances:\n",
      "[  9.69410081e-06   7.30380287e-09   3.55446498e-06   1.12749225e-05\n",
      "   9.12512217e-06   9.43033715e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.03918737e-08\n",
      "   0.00000000e+00   0.00000000e+00   5.01360189e-07   3.76096099e-05\n",
      "   3.62130966e-07   3.48174690e-05   4.52940759e-07   4.30120369e-06\n",
      "   1.59415288e-05   3.98929293e-05   2.83428319e-05   6.21431031e-06\n",
      "   1.73327167e-06   1.06114461e-04   1.88578620e-05   3.52744681e-05\n",
      "   6.97485105e-04   6.58572049e-05   1.10226151e-04   8.22970409e-05\n",
      "   6.08673159e-04   4.42119882e-04   7.44456892e-04   2.14831260e-03\n",
      "   6.49727906e-05   2.08191061e-04   6.73552704e-04   1.04822448e-03\n",
      "   2.72732267e-04   7.06280642e-04   8.80511120e-04   1.67221435e-03\n",
      "   1.52770775e-06   8.54630022e-05   4.83406264e-05   6.35892321e-05\n",
      "   1.76459172e-05   5.48312833e-05   6.14332033e-05   7.10933743e-06\n",
      "   0.00000000e+00   4.10071906e-03   4.12734358e-03   1.67994805e-02\n",
      "   1.17311068e-02   6.88270101e-03   5.32825664e-03   5.73747477e-03\n",
      "   4.98033680e-02   1.05890933e-02   9.76348127e-03   7.66419513e-03\n",
      "   2.88780115e-02   2.76243275e-02   1.20350153e-01   5.55004009e-03\n",
      "   1.95968371e-02   3.72010576e-02   5.43668952e-03   4.47838817e-06\n",
      "   1.27799228e-05   8.80911015e-06   8.40405227e-06   1.85385142e-05\n",
      "   1.02242776e-05   5.40483718e-06   1.90088219e-07   4.38245222e-05\n",
      "   3.13005322e-08   0.00000000e+00   0.00000000e+00   2.34512908e-07\n",
      "   3.12914504e-07   1.71298428e-06   2.43509353e-08   4.25929750e-08\n",
      "   0.00000000e+00   1.76140878e-07   3.46889168e-05   2.16685660e-05\n",
      "   8.67677971e-05   8.35377960e-05   2.67831095e-05   1.81189602e-06\n",
      "   4.38711095e-07   1.10228449e-04   1.69765038e-04   5.76548395e-05\n",
      "   6.05587169e-04   5.52422456e-05   1.49986315e-04   2.46344327e-04\n",
      "   4.62721800e-04   2.22510235e-04   5.07496307e-04   2.65687746e-03\n",
      "   3.55967910e-05   7.45834536e-04   4.75688207e-04   8.52708028e-04\n",
      "   2.20232085e-04   9.00303697e-04   1.16044467e-03   1.95517033e-03\n",
      "   5.39742179e-06   1.28679982e-04   6.00678171e-05   1.15025017e-05\n",
      "   2.95418300e-05   3.28872106e-05   2.89910271e-04   8.12339020e-07\n",
      "   2.78404536e-03   3.69350338e-03   1.40878136e-02   8.08652613e-03\n",
      "   4.49991570e-03   5.70932796e-03   9.20309999e-03   2.25910207e-02\n",
      "   6.78787244e-03   7.64117495e-03   9.11961026e-03   1.03161864e-02\n",
      "   1.03524097e-02   1.64135519e-02   4.59618636e-03   1.12138727e-02\n",
      "   2.22240964e-02   1.03393679e-02   5.29119653e-05   2.38200720e-06\n",
      "   1.28612013e-05   4.67895314e-06   4.77428726e-06   2.43047590e-05\n",
      "   1.71915898e-05   3.16358145e-06   1.25442822e-09   2.39669603e-07\n",
      "   6.98443819e-09   0.00000000e+00   0.00000000e+00   4.89015811e-07\n",
      "   8.73613525e-08   1.90534329e-07   0.00000000e+00   1.22167829e-06\n",
      "   9.87456145e-08   1.58930973e-07   4.18123194e-06   1.90085791e-05\n",
      "   5.83717311e-07   1.17059108e-05   1.82152183e-05   7.41087783e-07\n",
      "   8.54264371e-07   7.51151353e-05   3.03967768e-05   3.39490086e-07\n",
      "   5.46370034e-04   7.68183759e-05   1.28155154e-04   1.80325598e-04\n",
      "   3.93539444e-04   2.22964480e-04   6.65588363e-04   2.40526423e-03\n",
      "   1.68145682e-04   3.52846140e-04   7.01365547e-04   1.42840296e-03\n",
      "   2.01024060e-04   3.86620671e-04   1.40845809e-03   2.27542228e-03\n",
      "   3.18796869e-09   2.23836499e-04   3.88446332e-05   3.77394851e-06\n",
      "   8.65841837e-06   2.90630137e-04   5.70416549e-07   4.65701264e-03\n",
      "   3.65390805e-03   9.87326167e-03   5.48078537e-03   6.38909206e-03\n",
      "   6.01728281e-03   3.84386352e-03   3.54704356e-02   3.95225920e-03\n",
      "   4.61882875e-03   1.93550864e-02   1.19227301e-02   7.44377772e-03\n",
      "   1.24181210e-02   5.54016343e-03   3.96494485e-02   9.18824708e-03\n",
      "   3.79682209e-03   4.55468618e-05   5.85834589e-05   4.10874022e-06\n",
      "   1.42242144e-05   3.67127861e-05   3.68535770e-06   7.03708649e-06\n",
      "   6.46621797e-06   1.62440044e-06   8.61746079e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.70234386e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   2.08998536e-08\n",
      "   4.99780834e-07   7.47868191e-09   1.40456516e-05   1.86206990e-07\n",
      "   2.23429152e-05   3.70910224e-06   2.14762029e-05   2.74308445e-05\n",
      "   7.89642757e-07   4.66493842e-05   6.44244542e-05   2.67241996e-05\n",
      "   5.09417656e-04   1.00527608e-04   1.19612613e-04   1.16377917e-04\n",
      "   4.54190695e-04   5.18593453e-04   8.32918490e-04   1.66492290e-03\n",
      "   5.43876595e-05   6.15925973e-04   3.65175262e-04   1.08988098e-03\n",
      "   1.65534770e-04   6.58050395e-04   7.80066523e-04   2.10092965e-03\n",
      "   5.26031020e-07   2.64280483e-04   3.29095497e-05   2.20960944e-05\n",
      "   4.35292496e-05   0.00000000e+00   4.30558855e-03   5.25547797e-03\n",
      "   6.60094169e-03   7.56639304e-03   4.19242476e-03   7.35111451e-03\n",
      "   6.66215664e-03   2.68127270e-02   7.49455049e-03   1.13764966e-02\n",
      "   5.44801965e-03   9.67506505e-03   1.97417352e-02   1.04368437e-02\n",
      "   5.00474777e-03   5.21901313e-02   3.20424704e-03   9.61678149e-03]\n"
     ]
    }
   ],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "    \n",
    "    print(id)\n",
    "    \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "        \n",
    "    #RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "    #                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, \n",
    "    #                      max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "    #                      bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \n",
    "    #                      warm_start=False)\n",
    "    \n",
    "    #regr = RandomForestRegressor(n_estimators=30, max_depth=max_depth, criterion='mse', random_state=0,\\\n",
    "    #                            min_samples_leaf=1)\n",
    "    \n",
    "    #AdaBoostRegressor(base_estimator=None, n_estimators=150, learning_rate=1.0, loss=’linear’, random_state=None)\n",
    "    \n",
    "    regr = AdaBoostRegressor(RandomForestRegressor(max_depth=6), n_estimators=200, learning_rate=0.01, \\\n",
    "                             loss='exponential', random_state=42)\n",
    "    \n",
    "    regr.fit(X_train_, y_train_.ravel())\n",
    "\n",
    "    y_hat = regr.predict(X_train_)\n",
    "    y_hat[ y_hat < 0] = 0\n",
    "    y_hat = np.around(y_hat).astype('int')\n",
    "    y_pred = regr.predict(X_valid_)\n",
    "    y_pred[ y_pred < 0] = 0\n",
    "    y_pred = np.around(y_pred).astype('int')\n",
    "\n",
    "    # scores are training mae, validation mae, variance\n",
    "    mae_train = mean_absolute_error(y_train_, y_hat)\n",
    "    mae_valid = mean_absolute_error(y_valid_, y_pred)\n",
    "    r2        = r2_score(y_valid_, y_pred)\n",
    "\n",
    "    this_score = abs(r2/mae_valid)\n",
    "    if id[:2] == 'iq':\n",
    "        if this_score > best_score['iq']:\n",
    "            best_score['iq'] = this_score\n",
    "            best_result['iq'] = (regr, id, mae_train, mae_valid, r2)\n",
    "    else:\n",
    "        if this_score > best_score['sj']:\n",
    "            best_score['sj'] = this_score\n",
    "            best_result['sj'] = (regr, id, mae_train, mae_valid, r2)\n",
    "    \n",
    "    #plt.scatter(y_valid, y_pred)\n",
    "    \n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('\\tBest dataset: {}'.format(best_result[city][1]))\n",
    "    #print('\\tBest model  : {}'.format(best_result[city][2]))\n",
    "    print('\\t\\tMAE train : %.2f' % best_result[city][2])\n",
    "    print('\\t\\tMAE valid : %.2f' % best_result[city][3])\n",
    "    print('\\t\\tVariance  : %.2f' % best_result[city][4])\n",
    "    print('Feature Importances:')\n",
    "    print(best_result[city][0].feature_importances_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iq_1_raw\n",
      "iq_1_minmax1\n",
      "iq_1_minmax2\n",
      "iq_2_raw\n",
      "iq_2_minmax1\n",
      "iq_2_minmax2\n",
      "iq_3_raw\n"
     ]
    }
   ],
   "source": [
    "best_score = dict()\n",
    "best_score['iq'] = 0 \n",
    "best_score['sj'] = 0 \n",
    "best_result = dict()\n",
    "for id in dataset.keys():\n",
    "    \n",
    "    print(id)\n",
    "    \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "    \n",
    "    X_train_ = X_train_.reshape(X_train_.shape[0], int(id[3:4]), int(X_train_.shape[1]/int(id[3:4])))\n",
    "    X_valid_ = X_valid_.reshape(X_valid_.shape[0], int(id[3:4]), int(X_valid_.shape[1]/int(id[3:4])))\n",
    "    \n",
    "    # build graph\n",
    "    regr = Sequential()\n",
    "    \n",
    "    #keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "    #              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "    #              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "    #              unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #              bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "    #              recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
    "    #              recurrent_dropout=0.0, implementation=1, return_sequences=False, \n",
    "    #              return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "\n",
    "    regr.add(GRU(X_train_.shape[2], input_shape=(X_train_.shape[1], X_train_.shape[2]),\\\n",
    "                  implementation=2, return_sequences=True))\n",
    "    regr.add(GRU(10))\n",
    "    regr.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activation='linear'))\n",
    "    regr.compile(loss='mae', optimizer='rmsprop')  # 'adam'\n",
    "    \n",
    "    # fit net\n",
    "    history = regr.fit(X_train_, y_train_, epochs=20, batch_size=2,\\\n",
    "                       validation_data=(X_valid_, y_valid_), verbose=0, shuffle=False)\n",
    "    \n",
    "    y_hat = regr.predict(X_train_)\n",
    "    y_hat[ y_hat < 0] = 0\n",
    "    y_hat = np.around(y_hat).astype('int')\n",
    "    y_pred = regr.predict(X_valid_)\n",
    "    y_pred[ y_pred < 0] = 0\n",
    "    y_pred = np.around(y_pred).astype('int')\n",
    "    \n",
    "    loss_train = history.history['loss'][-1]\n",
    "    loss_valid = history.history['val_loss'][-1]\n",
    "    r2 = r2_score(y_valid_, y_pred)\n",
    "\n",
    "    if loss_valid > loss_train:\n",
    "        this_score = abs(r2/loss_valid)\n",
    "    else:\n",
    "        this_score = 0\n",
    "    \n",
    "    if id[:2] == 'iq':\n",
    "        if this_score > best_score['iq']:\n",
    "            best_score['iq'] = this_score\n",
    "            best_result['iq'] = history.history\n",
    "    else:\n",
    "        if this_score > best_score['sj']:\n",
    "            best_score['sj'] = this_score\n",
    "            best_result['sj'] = history.history\n",
    "        \n",
    "for city in best_result.keys():\n",
    "    print('Best result for {}'.format(city))\n",
    "    print('\\tFinal loss train: {}'.format(loss_train))\n",
    "    print('\\tFinal loss valid: {}'.format(loss_valid))\n",
    "    print('\\tVariance score: %.2f' % r2_score(y_valid_, y_pred))\n",
    "    plt.plot(best_result[city]['loss'], label='train')\n",
    "    plt.plot(best_result[city]['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
