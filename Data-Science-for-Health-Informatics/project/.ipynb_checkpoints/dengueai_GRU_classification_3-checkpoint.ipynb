{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import theano \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import myutil_gru_class as myutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and do preliminary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx_train = myutil.get_indexed_dataset('data/dengue_features_train.csv')\n",
    "dfy_train = myutil.get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "dfx_test = myutil.get_indexed_dataset('data/dengue_features_test.csv')\n",
    "# combine training features with training labels for data exploration later on\n",
    "dftrain = myutil.set_index(pd.merge(dfx_train, dfy_train))\n",
    "# Will stack the train and test datasets to treat all NaN values together\n",
    "# Need to add bogus total_cases column to test dataset so the files can be easily concatenated\n",
    "# update total_cases = -1 to easily identify the records for later split data to original partitions\n",
    "dfx_test['total_cases'] = -1\n",
    "dfall = myutil.set_index(pd.concat((dftrain, dfx_test), axis=0))\n",
    "dfall.sort_index(axis=0, inplace=True)\n",
    "# drop unecessary columns and save column names\n",
    "delcols = ['year','week_start_date','reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg']\n",
    "dfall.drop(delcols, axis=1, inplace=True)\n",
    "cols = dfall.columns\n",
    "# remove NaNs\n",
    "dfall = myutil.set_nan_to_week_mean(dfall.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slip dataset by city and One hot encode city and weekofyear +\n",
    "### Create several versions of the datasets (total cases is ONLY feature) +\n",
    "### Switch to Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataset and hot encode weekof year\n",
    "dfall_copy = dfall.copy()\n",
    "dfall_iq = dfall_copy[dfall_copy['city'] == 'iq']\n",
    "dfall_sj = dfall_copy[dfall_copy['city'] == 'sj']\n",
    "enc = OneHotEncoder(categorical_features=np.array([0]))\n",
    "dset = dict()\n",
    "dset['iq'] = enc.fit_transform(dfall_iq.iloc[:,1:].values).toarray()\n",
    "dset['sj'] = enc.fit_transform(dfall_sj.iloc[:,1:].values).toarray()\n",
    "\n",
    "# for each city, create version of datasets with 1, 2, 4, 8, 12 shifts\n",
    "for city in dset.keys():\n",
    "    datashift = dict()\n",
    "    for shift_no in [1,2,4,8,12]:\n",
    "        # total cases is THE ONLY FEATURE\n",
    "        datashift[shift_no] = myutil.shift(np.hstack((dset[city][:,-1:],dset[city][:,-1:])), shift_no)\n",
    "    dset[city] = datashift\n",
    "\n",
    "# for each city, each shift, create version of the dataset different scaling\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        vers = dict()\n",
    "        # save original as 'raw'\n",
    "#        vers['raw'] = np.hstack((dset[city][shift_no][:,:-1], dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#        vers['minmax1'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        # save scaled with minmax in range [0,1] as 'minmax1'\n",
    "        # isolate features, so scaling will only affect features\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        vers['minmax2'] = np.hstack((scaler.fit_transform(dset[city][shift_no][:,:-1]), dset[city][shift_no][:,-1:]))\n",
    "        dset[city][shift_no] = vers\n",
    "        \n",
    "# for each city, each shift, each version, create X_train, y_train, and X_test partitions\n",
    "dataset = dict()\n",
    "for city in dset.keys():\n",
    "    for shift_no in dset[city].keys():\n",
    "        for vers in dset[city][shift_no].keys():\n",
    "            npdata = dset[city][shift_no][vers]\n",
    "            partition = dict()\n",
    "            partition['X_train'] = npdata[npdata[:,-1]>0][:,:-1]\n",
    "            partition['y_train'] = npdata[npdata[:,-1]>0][:,-1:]\n",
    "            partition['X_test']  = npdata[npdata[:,-1]<0][:,:-1]\n",
    "            id = city + '_' + str(shift_no).zfill(2) + '_' + vers\n",
    "            dataset[id] = partition\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Switch to classification problem\n",
    "\n",
    "def get_class_from_value(total_cases, actual_to_class):   \n",
    "    cls = 1\n",
    "    for interval in actual_to_class.keys():\n",
    "        if total_cases in interval:\n",
    "            cls = actual_to_class[interval]\n",
    "    #print(actual_to_class)\n",
    "    #print('got {}, will return {}'.format(total_cases, cls))\n",
    "    return int(cls)\n",
    "   \n",
    "def get_value_from_class(source_class, class_to_actual): \n",
    "    val = 1\n",
    "    for cls in class_to_actual.keys():\n",
    "        if source_class == cls:\n",
    "            val = np.floor(class_to_actual[cls].mid)\n",
    "    return val\n",
    "\n",
    "def get_class_mapping(x, bins):\n",
    "    \n",
    "    y = pd.cut(x, precision=0, bins=bins)\n",
    "    \n",
    "    class_to_actual = dict()\n",
    "    actual_to_class = dict()\n",
    "    i = 0\n",
    "    for interval in y.categories:\n",
    "        class_to_actual[i] = interval\n",
    "        actual_to_class[interval] = int(i)\n",
    "        i += 1\n",
    "        \n",
    "    return class_to_actual, actual_to_class\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# y values are the same for all datasets (for each city)\n",
    "regr_value_iq = np.copy(dataset['iq_01_minmax2']['y_train'])\n",
    "regr_value_sj = np.copy(dataset['sj_01_minmax2']['y_train'])\n",
    "class_value_iq = np.copy(regr_value_iq)\n",
    "class_value_sj = np.copy(regr_value_sj)\n",
    "\n",
    "# set class mapping dictionaries\n",
    "class_to_regr_iq = dict()\n",
    "regr_to_class_iq = dict()\n",
    "class_to_regr_sj = dict()\n",
    "regr_to_class_sj = dict()\n",
    "class_to_regr_iq, regr_to_class_iq = get_class_mapping(np.copy(regr_value_iq).reshape(-1), classes)\n",
    "class_to_regr_sj, regr_to_class_sj = get_class_mapping(np.copy(regr_value_sj).reshape(-1), classes)\n",
    "\n",
    "# convert regr to class for iq\n",
    "i = 0\n",
    "for regr in list(regr_value_iq):\n",
    "    class_value_iq[i] = get_class_from_value(regr, regr_to_class_iq)\n",
    "    i += 1\n",
    "    \n",
    "# convert regr to class for sj\n",
    "i = 0\n",
    "for regr in list(regr_value_sj):\n",
    "    class_value_sj[i] = get_class_from_value(regr, regr_to_class_sj)\n",
    "    i += 1\n",
    "\n",
    "for key in dataset.keys():\n",
    "    #print(key)\n",
    "    if key[:2] == 'iq':\n",
    "        dataset[key]['y_train'] = np.copy(class_value_iq[-dataset[key]['X_train'].shape[0]:,:])\n",
    "    else:\n",
    "        dataset[key]['y_train'] = np.copy(class_value_sj[-dataset[key]['X_train'].shape[0]:,:])\n",
    "\n",
    "for key in dataset.keys():\n",
    "    #print(key)\n",
    "    if dataset[key]['X_train'].shape[0] != dataset[key]['y_train'].shape[0]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for key in dataset.keys():\n",
    "#    print(key)\n",
    "#    if dataset[key]['X_train'].shape[0] != dataset[key]['y_train'].shape[0]:\n",
    "#        print(key)\n",
    "                \n",
    "#dataset['iq_1_minmax2']['y_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def GRU_run(X, y, timesteps, reg, classes, epochs=50, batch_size=32, exploring=False):\n",
    "    \n",
    "    # create partitions for training\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    if exploring: print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], timesteps, int(X_train.shape[1]/timesteps))\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], timesteps, int(X_valid.shape[1]/timesteps)) \n",
    "    \n",
    "    data_dim = X_train.shape[2]\n",
    "\n",
    "    # build graph\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(X_train.shape[2]*timesteps, return_sequences=True, input_shape=(timesteps, X_train.shape[2])))  \n",
    "    model.add(GRU(X_train.shape[2], return_sequences=True))    \n",
    "    model.add(GRU(classes))                           \n",
    "    model.add(Dense(classes, activation='softmax', kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    # fit net\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_valid, y_valid), verbose=0, shuffle=False)\n",
    "    \n",
    "    y_hat  = np.argmax(model.predict(X_train), axis=1)\n",
    "    y_pred = np.argmax(model.predict(X_valid), axis=1)\n",
    "    \n",
    "    y_train.shape = y_hat.shape\n",
    "    y_train = y_train.astype('int')\n",
    "    print(\"y_train\", y_train)\n",
    "    print(\"y_hat\", y_hat)\n",
    "    \n",
    "    y_valid.shape = y_pred.shape\n",
    "    y_valid = y_valid.astype('int')\n",
    "    print(\"y_valid\", y_valid)\n",
    "    print(\"y_pred\", y_pred)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train.astype('str'), y_hat.astype('str'))\n",
    "    acc_valid = accuracy_score(y_valid.astype('str'), y_pred.astype('str'))\n",
    "    #r2        = r2_score(y_valid, y_pred)\n",
    "    print('Accurary train: {}'.format(acc_train))\n",
    "    print('Accurary valid: {}'.format(acc_valid))\n",
    "    #print('R2 score      : {}'.format(r2))\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    \n",
    "    if exploring:\n",
    "        print(\"Final loss train: {}\".format(history.history['loss'][-1]))\n",
    "        print(\"Final loss valid: {}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get test dataset, create predictions and save them in the proper submission file format\n",
    "def GRU_predict_and_save(df_iq, model_iq, df_sj, model_sj, dftest_iq, dftest_sj, filename):\n",
    "\n",
    "    yhat_iq = model_iq.predict(nptest_iq.reshape(nptest_iq.shape[0], ts_iq, int(nptest_iq.shape[1]/ts_iq)))\n",
    "    yhat_sj = model_sj.predict(nptest_sj.reshape(nptest_sj.shape[0], ts_sj, int(nptest_sj.shape[1]/ts_sj)))\n",
    "    \n",
    "    print(yhat_iq.shape)\n",
    "    print(yhat_sj.shape)\n",
    "\n",
    "    dfsubm = pd.read_csv('data/submission_format.csv')\n",
    "    npsubm_sj = np.concatenate((dfsubm[dfsubm['city']=='sj'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_sj.round().astype('int64')), axis=1)\n",
    "    npsubm_iq = np.concatenate((dfsubm[dfsubm['city']=='iq'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_iq.round().astype('int64')), axis=1)\n",
    "    dfresults = pd.DataFrame(np.concatenate((npsubm_sj, npsubm_iq), axis=0), columns=dfsubm.columns)\n",
    "    dfresults.to_csv(filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "iq_01_minmax2\n",
      "=================================\n",
      "\n",
      "(284, 1) (140, 1) (284, 1) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "for id in dataset.keys():\n",
    "    \n",
    "    print('=================================\\n' + id + '\\n=================================\\n')\n",
    "    \n",
    "    myscore = dict()\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(dataset[id]['X_train'], dataset[id]['y_train'],\\\n",
    "                                                              test_size=.33, random_state=42)\n",
    "    \n",
    "    models[id] = GRU_run(dataset[id]['X_train'], dataset[id]['y_train'], timesteps=int(id[3:5]),\\\n",
    "                        reg=0.2, classes=10, epochs=30, batch_size=2, exploring=True)\n",
    "\n",
    "\n",
    "#Xy_test = Xy_test.reshape(Xy_test.shape[0], 1, Xy_test.shape[1])\n",
    "#pred_iq = np.argmax(model_iq.predict(Xy_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test dataset and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
