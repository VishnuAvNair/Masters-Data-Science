{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import statsmodels as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "%matplotlib inline\n",
    "import myutil_lstm as myutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "_ = importlib.reload(myutil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and take first look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_train = myutil.get_indexed_dataset('data/dengue_features_train.csv')\n",
    "dfy_train = myutil.get_indexed_dataset('data/dengue_labels_train.csv')\n",
    "dfx_test = myutil.get_indexed_dataset('data/dengue_features_test.csv')\n",
    "# combine training features with training labels for data exploration later on\n",
    "dftrain = myutil.set_index(pd.merge(dfx_train, dfy_train))\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with NaN on both training and test datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dftrain.dtypes\n",
    "#dftrain.isnull().sum()\n",
    "#dftest.isnull().sum()\n",
    "# Will stack the train and test datasets to treat all NaN values together\n",
    "# Need to add bogus total_cases column to test dataset so the files can be easily concatenated\n",
    "# update total_cases = -1 to easily identify the records for later split data to original partitions\n",
    "dfx_test['total_cases'] = -1\n",
    "dfall = myutil.set_index(pd.concat((dftrain, dfx_test), axis=0))\n",
    "dfall.sort_index(axis=0, inplace=True)\n",
    "dfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfall = myutil.set_nan_to_week_mean(dfall.copy())\n",
    "dfall.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfall_iq, dfall_sj = myutil.split_dataset_by_city(dfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "def drop_columns(df):\n",
    "    df.drop(['city','year','week_start_date'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "dfall_iq = drop_columns(dfall_iq.copy())\n",
    "dfall_sj = drop_columns(dfall_sj.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore training and test partitions (now that NaNs have been properly filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain_iq = dfall_iq[dfall_iq['total_cases']>0].copy()     # total_cases was set to -1 for test partition\n",
    "dftrain_sj = dfall_sj[dfall_sj['total_cases']>0].copy()     # total_cases was set to -1 for test partition\n",
    "\n",
    "dftest_iq = dfall_iq[dfall_iq['total_cases']<0].copy()\n",
    "dftest_sj = dfall_sj[dfall_sj['total_cases']<0].copy()\n",
    "dftest_iq.drop('total_cases', axis=1, inplace=True)\n",
    "dftest_sj.drop('total_cases', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split dataset into test and validation partitions\n",
    "def prep_LSTM_run(city_data, timesteps):\n",
    "    \n",
    "    X = city_data[:,:-1]\n",
    "    y = city_data[:,-1]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    # LSTM exped 3D input of the form [no_samples, timesteps, no_features] \n",
    "    X_train = X_train.reshape(X_train.shape[0], timesteps, int(X_train.shape[1]/timesteps))\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], timesteps, int(X_valid.shape[1]/timesteps))\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def LSTM_run_iq(nptrain, timesteps=1, epochs=50, batch_size=32, exploring=False):\n",
    "    \n",
    "    # create partitions for training\n",
    "    X_train, X_valid, y_train, y_valid = prep_LSTM_run(nptrain, timesteps)\n",
    "    if exploring: print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    # build graph\n",
    "    model = Sequential()\n",
    "    \n",
    "    #keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "    #              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "    #              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "    #              unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #              bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "    #              recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
    "    #              recurrent_dropout=0.0, implementation=1, return_sequences=False, \n",
    "    #              return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "\n",
    "    model.add(LSTM((X_train.shape[1]*X_train.shape[2])*2, return_sequences=True,\\\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(X_train.shape[1]*X_train.shape[2], return_sequences=True))\n",
    "    model.add(LSTM(X_train.shape[1], return_sequences=False))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mae', optimizer='rmsprop')  # 'rmsprop'\n",
    "    \n",
    "    # fit net\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_valid, y_valid), verbose=0, shuffle=False)\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    \n",
    "    if exploring:\n",
    "        print(\"Final loss train: {}\".format(history.history['loss'][-1]))\n",
    "        print(\"Final loss valid: {}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get test dataset, create predictions and save them in the proper submission file format\n",
    "def LSTM_predict_and_save(df_iq, model_iq, ts_iq, df_sj, model_sj, ts_sj, dftest_iq, dftest_sj, filename):\n",
    "\n",
    "    nptest_iq = myutil.preprocess_test(df_iq.copy(), dftest_iq, ts_iq)\n",
    "    nptest_sj = myutil.preprocess_test(df_sj.copy(), dftest_sj, ts_sj)\n",
    "\n",
    "    yhat_iq = model_iq.predict(nptest_iq.reshape(nptest_iq.shape[0], ts_iq, int(nptest_iq.shape[1]/ts_iq)))\n",
    "    yhat_sj = model_sj.predict(nptest_sj.reshape(nptest_sj.shape[0], ts_sj, int(nptest_sj.shape[1]/ts_sj)))\n",
    "    \n",
    "    #print(yhat_iq.shape)\n",
    "    #print(yhat_sj.shape)\n",
    "\n",
    "    dfsubm = pd.read_csv('data/submission_format.csv')\n",
    "    npsubm_sj = np.concatenate((dfsubm[dfsubm['city']=='sj'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_sj.round().astype('int64')), axis=1)\n",
    "    npsubm_iq = np.concatenate((dfsubm[dfsubm['city']=='iq'][['city','year','weekofyear']].values, \\\n",
    "                                yhat_iq.round().astype('int64')), axis=1)\n",
    "    dfresults = pd.DataFrame(np.concatenate((npsubm_sj, npsubm_iq), axis=0), columns=dfsubm.columns)\n",
    "    dfresults.to_csv(filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods_iq = 2\n",
    "nptrain_iq = myutil.preprocess(dftrain_iq.copy(), periods_iq)\n",
    "model_iq = LSTM_run_iq(nptrain_iq, timesteps=periods_iq, epochs=25, batch_size=periods_iq*2, exploring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_run_sj(nptrain, timesteps=1, epochs=50, batch_size=32, exploring=False):\n",
    "    \n",
    "    # create partitions for training\n",
    "    X_train, X_valid, y_train, y_valid = prep_LSTM_run(nptrain, timesteps)\n",
    "    if exploring: print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
    "    \n",
    "    # build graph\n",
    "    model = Sequential()\n",
    "    \n",
    "    #keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "    #              use_bias=True, kernel_initializer='glorot_uniform', \n",
    "    #              recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "    #              unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #              bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "    #              recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
    "    #              recurrent_dropout=0.0, implementation=1, return_sequences=False, \n",
    "    #              return_state=False, go_backwards=False, stateful=False, unroll=False)\n",
    "\n",
    "    model.add(LSTM((X_train.shape[1]*X_train.shape[2])*4, return_sequences=True,\\\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM((X_train.shape[1]*X_train.shape[2])*2, return_sequences=True))\n",
    "    model.add(LSTM((X_train.shape[1]*X_train.shape[2])*1, return_sequences=True))\n",
    "    model.add(LSTM(X_train.shape[1], return_sequences=False))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mae', optimizer='rmsprop')  # 'rmsprop'\n",
    "    \n",
    "    # fit net\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(X_valid, y_valid), verbose=0, shuffle=False)\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    \n",
    "    if exploring:\n",
    "        print(\"Final loss train: {}\".format(history.history['loss'][-1]))\n",
    "        print(\"Final loss valid: {}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods_sj = 2\n",
    "nptrain_sj = myutil.preprocess(dftrain_sj.copy(), periods_sj)\n",
    "model_sj = LSTM_run_sj(nptrain_sj, timesteps=periods_sj, epochs=60, batch_size=periods_iq*2, exploring=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test dataset and create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_predict_and_save(dftrain_iq, model_iq, periods_iq, dftrain_sj, model_sj, periods_sj, dftest_iq, dftest_sj,\\\n",
    "                      \"data/submission_20171108_lstm_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
